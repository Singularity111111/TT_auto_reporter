# -*- coding: utf-8 -*-
"""
generate_daily_agent_report.py

å°†åŒä¸€ç›®å½•ä¸‹çš„å¤šç±» CSV/XLSXï¼ˆè¿è¥/ä»£ç†/å¹³å°/ç”¨æˆ·æ—¥å¸¸/ç•™å­˜/é¦–å……LTV/æˆæœ¬ç­‰ï¼‰
æŒ‰ç»Ÿä¸€å£å¾„ æ¸…æ´—â†’å¯¹é½â†’åˆå¹¶â†’è®¡ç®—ï¼Œç”Ÿæˆ"æ¯æ—¥æ€»ä»£æ•°æ®"ï¼ˆæ¸ é“/æ€»ä»£æ—¥çº§ï¼Œä¸»é”®ï¼š[æ—¥æœŸ, ç›˜å£, æ€»ä»£å·]ï¼‰ã€‚

ä½¿ç”¨ï¼š
  python generate_daily_agent_report.py --input . --output æ¯æ—¥æ€»ä»£æ•°æ®_è‡ªåŠ¨ç”Ÿæˆ.xlsx


- ä¸ç›®æ ‡è¡¨å¯¹é½çš„ 53 åˆ—å·²ç»åœ¨é…ç½®åŒºçš„ FINAL_COLUMNS ä¸­å®šä¹‰ï¼Œå¯è‡ªç”±è°ƒæ•´é¡ºåºã€‚
- "æ€»ä»£å·"æ¥è‡ªã€è¿è¥æ•°æ®ã€‘é‡Œ"æ€»ä»£åç§°/æ¸ é“/channel/agent_name"å­—æ®µæœ«å°¾æ‹¬å·çš„çº¯æ•°å­—ï¼ˆæ”¯æŒåŠè§’/å…¨è§’ï¼‰ï¼Œ
  å…¶å®ƒæ¥æºè‹¥æ—  IDï¼ŒæŒ‰"æ¸…æ´—åçš„æ€»ä»£åç§°"æ˜ å°„å›å¡«ã€‚
- å¹³å°/ç›˜å£çº§æŒ‡æ ‡ï¼ˆLTV/ç•™å­˜/æˆæœ¬ç­‰ï¼‰æŒ‰ [æ—¥æœŸ, ç›˜å£] å¹¿æ’­åˆ°æ‰€æœ‰æ¸ é“è¡Œã€‚
- æ— å¯¹åº”æ¥æºæ—¶ï¼Œæ•°å€¼é»˜è®¤ 0ï¼ˆä¸ªåˆ«å­—æ®µç•™ç©ºï¼‰ï¼Œä»¥ä¿è¯ 53 åˆ—å®Œæ•´è¾“å‡ºã€‚
"""

import argparse, os, sys, re, glob
from datetime import datetime
from hashlib import md5
import pandas as pd

# è®¾ç½®è¾“å‡ºç¼–ç ä¸º UTF-8ï¼ˆè§£å†³ Windows æ§åˆ¶å°ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ï¼‰
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

# ----------------------------- å¯é…ç½®åŒºåŸŸï¼ˆå¿…è¦æ—¶è¡¥å……ï¼‰ -----------------------------

# ğŸ”§ é…ç½®ï¼šæ±‡ç‡ï¼ˆåç»­å¯è°ƒæ•´ï¼‰
EXCHANGE_RATES = {
    "å·´è¥¿": 6.0,
    "å¢¨è¥¿å“¥": 18.7,
    # åç»­å¯æ·»åŠ å…¶ä»–åœ°åŒº
}

# ğŸ”§ é…ç½®ï¼šæŒ‡å®šè¦ç”Ÿæˆçš„æ—¥æœŸ
# - è®¾ç½®ä¸º None æˆ– "latest"ï¼šè‡ªåŠ¨ä½¿ç”¨æœ€æ–°æ—¥æœŸ
# - è®¾ç½®ä¸ºå…·ä½“æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆå¦‚ "2025-10-28"ï¼‰ï¼šåªå¤„ç†è¯¥æ—¥æœŸçš„æ•°æ®
# - å¯é€šè¿‡å‘½ä»¤è¡Œå‚æ•° --date è¦†ç›–æ­¤é…ç½®
TARGET_DATE = "2025-10-30"  # æŒ‡å®šå…·ä½“æ—¥æœŸï¼ˆæ¨èï¼‰

# ğŸ”§ é…ç½®ï¼šæ‰¹é‡ç”Ÿæˆæ¨¡å¼ï¼ˆå¯é€‰ï¼‰
# - è®¾ç½®ä¸º True æ—¶ï¼Œç”Ÿæˆæ—¥æœŸåŒºé—´å†…çš„å¤šä¸ªæŠ¥è¡¨
BATCH_MODE = False  # True æ—¶å¯ç”¨æ‰¹é‡ç”Ÿæˆ
BATCH_START_DATE = "2025-10-20"  # æ‰¹é‡èµ·å§‹æ—¥æœŸ
BATCH_END_DATE = "2025-10-28"  # æ‰¹é‡ç»“æŸæ—¥æœŸ

# ğŸ”§ é…ç½®ï¼šä¸€çº§é¦–å……ä¸åç§»è®¡ç®—å£å¾„
# - PRIMARY_FIRSTPAY_SOURCE: "all" ä½¿ç”¨è£‚å˜ç±»å‹=å…¨éƒ¨ï¼›"parent" ä½¿ç”¨è£‚å˜ç±»å‹=parent
PRIMARY_FIRSTPAY_SOURCE = "all"
# - OFFSET_MODE: "retention" ç›´æ¥ä½¿ç”¨ç•™å­˜ç‡ï¼›"formula" ä½¿ç”¨ç•™å­˜ç‡Ã—(é¦–å……äººæ•°/æ³¨å†Œäººæ•°)
OFFSET_MODE = "formula"
# - WITHDRAW_APPROX_MODE: "scale" æŒ‰æ¯”ä¾‹ä¼°ç®—é¦–å……ç”¨æˆ·æç°ï¼›"zero" ä¸ä¼°ç®—ï¼ˆé¦–å……å……æå·®=é¦–å……é‡‘é¢ï¼‰
WITHDRAW_APPROX_MODE = "scale"

# ğŸ”§ é…ç½®ï¼šè¾“å‡ºå­—æ®µé¡ºåºï¼ˆå¯è°ƒæ•´ï¼‰
# - ä¿®æ”¹æ­¤åˆ—è¡¨å¯ä»¥è°ƒæ•´æœ€ç»ˆæŠ¥è¡¨çš„åˆ—é¡ºåº
# - æ³¨æ„ï¼šå¿…é¡»åŒ…å«æ‰€æœ‰53ä¸ªå­—æ®µï¼Œä¸”å­—æ®µåå¿…é¡»å®Œå…¨åŒ¹é…
FINAL_COLUMNS = [
    "äº§å“","ç›˜å£","æ—¥æœŸ","æ€»ä»£å·","æ€»ä»£åç§°","æ¨å¹¿éƒ¨é—¨","æ¨å¹¿æ–¹å¼","æ¶ˆè€—","å±•ç¤º","ç‚¹å‡»","åƒå±•æˆæœ¬crm","ç‚¹å‡»ç‡",
    "æ³¨å†Œæˆæœ¬","é¦–å……æˆæœ¬","ä¸€çº§é¦–å……æˆæœ¬","æ³¨å†Œäººæ•°","é¦–å……äººæ•°","ä¸€çº§é¦–å……äººæ•°","é¦–å……è½¬åŒ–ç‡","é¦–å……arppu","é¦–å……roas",
    "é¦–å……å½“æ—¥ltv","é¦–å……å½“æ—¥roi","é¦–å……å……æå·®æ¯”","å½“æ—¥é¦–å……é‡‘é¢","é¦–å……å½“æ—¥å……æå·®",
    "é¦–å……æ¬¡æ—¥å¤ç™»ç‡_åç§»","é¦–å……ä¸‰æ—¥å¤ç™»ç‡_åç§»","é¦–å……ä¸ƒæ—¥å¤ç™»ç‡_åç§»","é¦–å……åäº”æ—¥å¤ç™»ç‡_åç§»","é¦–å……ä¸‰åæ—¥å¤ç™»ç‡_åç§»",
    "é¦–å……æ¬¡æ—¥å¤æŠ•ç‡_åç§»","é¦–å……ä¸‰æ—¥å¤æŠ•ç‡_åç§»","é¦–å……ä¸ƒæ—¥å¤æŠ•ç‡_åç§»","é¦–å……åäº”æ—¥å¤æŠ•ç‡_åç§»","é¦–å……ä¸‰åæ—¥å¤æŠ•ç‡_åç§»",
    "é¦–å……æ¬¡æ—¥å¤å……ç‡_åç§»","é¦–å……ä¸‰æ—¥å¤å……ç‡_åç§»","é¦–å……ä¸ƒæ—¥å¤å……ç‡_åç§»","é¦–å……åäº”æ—¥å¤å……ç‡_åç§»","é¦–å……ä¸‰åæ—¥å¤å……ç‡_åç§»",
    "é¦–å……ä¸¤æ—¥ltv_åç§»","é¦–å……ä¸‰æ—¥ltv_åç§»","é¦–å……ä¸ƒæ—¥ltv_åç§»","é¦–å……åäº”æ—¥ltv_åç§»","é¦–å……ä¸‰åæ—¥ltv_åç§»",
    "ç´¯è®¡roas","è‡ªç„¶æœˆæ¶ˆè€—","éä¸€çº§é¦–å……äººæ•°/é¦–å……äººæ•°","éä¸€çº§é¦–å……äººæ•°/å……å€¼äººæ•°","å……å€¼é‡‘é¢","å……å€¼äººæ•°","å……æå·®"
]

# ğŸ”§ é…ç½®ï¼šå¹³å°â†’æ¨å¹¿éƒ¨é—¨æ˜ å°„ï¼ˆV3.6æ–°å¢ï¼‰
# - ä»æ–‡ä»¶åæå–å¹³å°åï¼Œé€šè¿‡æ­¤æ˜ å°„è·å–æ¨å¹¿éƒ¨é—¨
PLATFORM_TO_DEPT = {
    "OK7": "å¤©æˆ",
    "58": "å¤©æˆ", 
    "AI7": "å¤©æˆ",
    "98": "å¤©æˆ",
    "LV7": "å¤©æˆ",
    "OO7": "å¤©æˆ",
    "Bmw7": "å¤©æˆ",
    "bmw7": "å¤©æˆ",  # å°å†™ç‰ˆæœ¬
    "U777": "è¡Œè¿œ",
    "u777": "è¡Œè¿œ",
    "ONE7": "å¼ºç››",
    "one7": "å¼ºç››",
    "17SS": "å¤©é¾™",
    "17ss": "å¤©é¾™",
    "1xspin": "A8",
    "7sortudo": "57",
    "novabr": "T9",
    "tp7": "å¤©é¾™",
    "SP1": "A8",
    "sp1": "A8",
    "pg7k": "KK",
    "5151": "å¤§æ²³",
    "BRL77": "A8",
    "brl77": "A8",
    "b777": "A8",
    "spin77": "A8",
    "brplay7": "A8",
    "hot77": "A8",
    "super7": "A8",
    "viva7": "A8",
    "gana7": "A8",
    "ak9": "T9",
    "samba": "T9",
    "bra1": "TTVSA8",
    "M333": "M7",
    "m333": "M7",
    "T33": "å¤©æˆ",
    "t33": "å¤©æˆ",
    "IV7": "TL",
    "iv7": "TL",
    "kq7": "TL",
    "b77": "å¼ºç››",
}

# åˆ—ååˆ«åæ˜ å°„ï¼ˆå„æ¥æº â†’ æ ‡å‡†å­—æ®µåï¼‰
ALIASES = {
    "date":        ["æ—¥æœŸ","æ—¶é—´","date","ç»Ÿè®¡æ—¥æœŸ","dt"],
    "channel":     ["æ¸ é“åç§°","æ€»ä»£åç§°","æ¸ é“","channel","agent_name","æ¸ é“å","agent","channel_name","ä»£ç†åç§°"],
    "agent_id":    ["ä»£ç†ID","agent_id","AgentID","æ€»ä»£ID"],  # v2.1: æ–°å¢ä»£ç†IDåˆ—è¯†åˆ«
    "platform":    ["ç›˜å£","å¹³å°","platform","game_platform"],
    # æŒ‡æ ‡ç±»
    "register":    ["æ³¨å†Œäººæ•°","æ–°å¢æ³¨å†Œ","æ³¨å†Œ"],
    "active":      ["æ´»è·ƒäººæ•°","æ´»è·ƒ"],
    "pay_users":   ["å……å€¼äººæ•°","ä»˜è´¹äººæ•°"],
    "pay_amount":  ["å……å€¼é‡‘é¢","ä»˜è´¹é‡‘é¢","é‡‘é¢","æ€»å……å€¼"],
    "firstpay_u":  ["é¦–å……äººæ•°","é¦–å……äººæ•°ï¼ˆå½“æ—¥ï¼‰"],
    "firstpay_a":  ["é¦–å……é‡‘é¢","é¦–å……é‡‘é¢ï¼ˆå½“æ—¥ï¼‰","é¦–å……ä»˜è´¹é‡‘é¢","å½“æ—¥é¦–å……é‡‘é¢"],
    "pay_active_u":["æ´»è·ƒå……å€¼äººæ•°"],
    "impr":        ["å±•ç¤º","impressions"],
    "click":       ["ç‚¹å‡»","clicks"],
    "spend":       ["æ¶ˆè€—","cost","èŠ±è´¹","spent"],
    "withdraw":    ["æç°é‡‘é¢","withdrew","withdrawal"],
    # è¿è¥ç±»ï¼ˆè‹¥æœ‰ï¼‰
    "bet_amt":     ["æŠ•æ³¨é‡‘é¢","æ€»æŠ•æ³¨é¢"],
    "win_amt":     ["ä¸­å¥–é‡‘é¢","æ€»ä¸­å¥–é¢"],
    "bet_cnt":     ["æŠ•æ³¨æ¬¡æ•°"],
    "bet_users":   ["æŠ•æ³¨äººæ•°"],
    # LTV & Retentionï¼ˆä¸åŒæ¥æºåˆ—åå¯èƒ½ä¸åŒï¼Œè¿™é‡Œåªç¤ºæ„ï¼›è„šæœ¬ä¼šæŒ‰å¸¸è§å‘½åæŠ“å–ï¼‰
    "ltv_d1":      ["LTV(D1)","ltv_d1","ltv1"],
    "ltv_d3":      ["LTV(D3)","ltv_d3","ltv3"],
    "ltv_d7":      ["LTV(D7)","ltv_d7","ltv7"],
    "ltv_d14":     ["LTV(D14)","ltv_d14","ltv14"],
    "ltv_d30":     ["LTV(D30)","ltv_d30","ltv30"],
    # é¦–å……LTVï¼ˆFPLTVï¼‰
    "fpltv_d1":    ["FPLTV_D1","é¦–å……å½“æ—¥LTV","fpltv_d1"],
    "fpltv_d2":    ["FPLTV_D2","fpltv_d2"],
    "fpltv_d3":    ["FPLTV_D3","fpltv_d3"],
    "fpltv_d7":    ["FPLTV_D7","fpltv_d7"],
    "fpltv_d15":   ["FPLTV_D15","fpltv_d15"],
    "fpltv_d30":   ["FPLTV_D30","fpltv_d30"],
    # ç•™å­˜ï¼ˆç¤ºä¾‹ï¼šD1/D3/D7/D14/D30ï¼›ä¸åŒæ¥æºæ–‡ä»¶ååŒºåˆ†ï¼šfirst_login / register / first_payï¼‰
    "ret_d1":      ["D1","ç•™å­˜ç‡(D1)","ret_d1"],
    "ret_d3":      ["D3","ç•™å­˜ç‡(D3)","ret_d3"],
    "ret_d7":      ["D7","ç•™å­˜ç‡(D7)","ret_d7"],
    "ret_d14":     ["D14","ç•™å­˜ç‡(D14)","ret_d14"],
    "ret_d30":     ["D30","ç•™å­˜ç‡(D30)","ret_d30"],
}

# ä»£ç æ˜ å°„ï¼ˆæ ¹æ®ä½ ä»¬çš„è§„åˆ™å›¾å¯è‡ªè¡Œè¡¥å…¨ï¼‰
TYPE_MAP   = {"111":"æŠ•æ”¾","222":"ç½‘çº¢","333":"ç¾¤å‘(çŸ­ä¿¡ç­‰)","444":"å¤–éƒ¨åˆä½œ","555":"ä»»åŠ¡é‡(å˜ç°)","666":"ç§åŸŸ","OP":"è¿è¥"}
MEDIA_MAP  = {"KKK":"FB","SSS":"å¿«æ‰‹","TK":"æŠ–éŸ³/TikTok","TTT":"Twitter","GGG":"è°·æ­Œ","III":"INS","QQQ":"å…¶ä»–","WS":"WS","ZZZ":"ç¾¤å‘","RRR":"bigo"}
METHOD_MAP = {"AAA":"H5","BBB":"PWA","CCC":"é©¬ç”²åŒ…","DDD":"è°·æ­ŒåŒ…","EEE":"APK","PPP":"iOSè‹¹æœåŒ…","FFF":"å°ç±³åŒ…"}

# éƒ¨é—¨â†’ç›˜å£ï¼ˆä½ ä»¬çš„æ˜ å°„ï¼ŒæŒ‰éœ€è¡¥å…¨ï¼›å¦‚æœå¹³å°æŠ¥è¡¨æœ‰ platform å­—æ®µå°±ä¼˜å…ˆç”¨å¹³å°å­—æ®µï¼‰
DEPT_TO_PLATFORM = {
    # ä¾‹ï¼š "A8":"A8",
}

# ----------------------------- å·¥å…·å‡½æ•° -----------------------------

def read_any_csv(path):
    """è‡ªåŠ¨è¯†åˆ«åˆ†éš”ç¬¦/ç¼–ç è¯»å– CSVã€‚V3.5.4: å¢å¼ºç¼–ç æ”¯æŒ + å®¹é”™å¤„ç†"""
    # V3.5.4: æ‰©å±•ç¼–ç åˆ—è¡¨ï¼Œè¦†ç›–æ›´å¤šå¸¸è§ç¼–ç 
    encodings = [
        "utf-8", 
        "utf-8-sig", 
        "gbk", 
        "latin-1",      # è¥¿æ¬§å­—ç¬¦é›†
        "cp1252",       # Windowsè¥¿æ¬§
        "iso-8859-1",   # ISOè¥¿æ¬§
        "gb2312",       # ç®€ä½“ä¸­æ–‡
    ]
    
    for enc in encodings:
        try:
            # sep=None + engine='python' å¯è‡ªåŠ¨æ¨æ–­åˆ†éš”ç¬¦
            df = pd.read_csv(path, sep=None, engine="python", dtype=str, encoding=enc)
            return df
        except Exception:
            continue
    
    # V3.5.4: æ‰€æœ‰ç¼–ç éƒ½å¤±è´¥æ—¶ï¼Œè¾“å‡ºè­¦å‘Šå¹¶è¿”å›ç©ºDataFrameï¼ˆè€Œä¸æ˜¯å´©æºƒï¼‰
    print(f"  âš ï¸ [è·³è¿‡æ–‡ä»¶] æ— æ³•è¯»å–ï¼ˆä¸æ”¯æŒçš„ç¼–ç ï¼‰: {os.path.basename(path)}")
    return pd.DataFrame()

def read_any_table(path):
    """æ ¹æ®æ‰©å±•åè‡ªåŠ¨è¯»å– CSV æˆ– Excelï¼ˆé¦–ä¸ªsheetï¼‰ï¼Œç»Ÿä¸€ä¸º DataFrame[str]ã€‚V3.5.4: å¢å¼ºå®¹é”™"""
    ext = os.path.splitext(path)[1].lower()
    if ext in [".csv", ".txt", ".tsv"]:
        return read_any_csv(path)
    # Excel
    try:
        df = pd.read_excel(path, dtype=str)
        return df
    except Exception:
        try:
            # å†è¯•ä¸€æ¬¡ä¸åŒå¼•æ“
            df = pd.read_excel(path, dtype=str, engine="openpyxl")
            return df
        except Exception as e:
            # V3.5.4: Excelè¯»å–å¤±è´¥æ—¶è¿”å›ç©ºDataFrame
            print(f"  âš ï¸ [è·³è¿‡æ–‡ä»¶] æ— æ³•è¯»å–Excel: {os.path.basename(path)} ({e})")
            return pd.DataFrame()

def is_valid_data_file(filepath, filename):
    """
    ç™½åå•ï¼šåªæ¥å—æœ‰æ•ˆçš„æ•°æ®æºæ–‡ä»¶ï¼ˆV3.5.8æ–°å¢ï¼‰
    
    è¿”å›Trueçš„æ–‡ä»¶ï¼š
      1. downloads/ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
      2. æ ¹ç›®å½•ä¸‹1xspingames_å¼€å¤´çš„æ–‡ä»¶ï¼ˆçˆ¬è™«ä¸‹è½½ï¼‰
      3. æ ¹ç›®å½•ä¸‹TT-å¼€å¤´çš„æ–‡ä»¶ï¼ˆçˆ¬è™«ä¸‹è½½ï¼‰
    
    è¿”å›Falseçš„æ–‡ä»¶ï¼ˆè¢«è¿‡æ»¤ï¼‰ï¼š
      - å†å²Excelæ–‡ä»¶ï¼ˆæ¯æ—¥æ€»ä»£æ•°æ®_XXX.xlsxï¼‰
      - é…ç½®æ–‡ä»¶ï¼ˆsites.csvã€é˜ˆå€¼è¥æ”¶è¡¨.xlsxï¼‰
      - å…¶ä»–éæ•°æ®æºæ–‡ä»¶
    """
    # 1. downloadsç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶éƒ½æ¥å—
    normalized_path = filepath.replace('\\', '/')
    if 'downloads' in normalized_path:
        return True
    
    # 2. æ ¹ç›®å½•ä¸‹åªæ¥å—çˆ¬è™«ä¸‹è½½çš„ç‰¹å®šæ ¼å¼
    # 1xspingameså¼€å¤´çš„æ–‡ä»¶
    if filename.startswith('1xspingames_'):
        return True
    
    # TT-å¼€å¤´çš„æ–‡ä»¶ï¼ˆçˆ¬è™«ä¸‹è½½çš„é‡å‘½åæ–‡ä»¶ï¼‰
    if filename.startswith('TT-'):
        return True
    
    # 3. å…¶ä»–ä¸€å¾‹æ‹’ç»ï¼ˆåŒ…æ‹¬å†å²Excelã€sites.csvã€é˜ˆå€¼è¥æ”¶è¡¨ç­‰ï¼‰
    return False

def is_meaningful_df(df, required_cols=None):
    """V3.5.8: åˆ¤æ–­DataFrameæ˜¯å¦æœ‰æ„ä¹‰ï¼ˆéç©ºã€éå…¨NAã€å¯é€‰å¿…éœ€åˆ—å­˜åœ¨ä¸”å«éç©ºå€¼ï¼‰"""
    if not isinstance(df, pd.DataFrame) or df.shape[1] == 0 or df.empty:
        return False
    if df.dropna(how='all').empty:
        return False
    if required_cols:
        for col in required_cols:
            if col not in df.columns:
                return False
        subset = df[required_cols]
        if subset.dropna(how='all').empty:
            return False
    return True

def consolidate_duplicate_columns(df):
    """
    V3.5.8: åˆå¹¶é‡å¤åˆ—åã€‚å¯¹åŒåå¤šåˆ—ï¼ŒæŒ‰è¡Œä¼˜å…ˆå–éç©ºå€¼ï¼ˆbfillï¼‰åˆå¹¶ä¸ºå•åˆ—ï¼Œç§»é™¤å¤šä½™åˆ—ã€‚
    è¿”å›ï¼šåˆå¹¶çš„ä¿¡æ¯å­—å…¸ {åˆ—å: åˆå¹¶åˆ—æ•°é‡}
    """
    from collections import Counter
    name_counts = Counter(df.columns)
    dup_names = [n for n, c in name_counts.items() if c > 1]
    merged_info = {}
    if dup_names:
        print(f"  [èšåˆå‰] æ£€æµ‹åˆ°é‡å¤åˆ—: {dup_names}")
    for name in dup_names:
        cols = [c for c in df.columns if c == name]
        # è¡Œå†…ä¼˜å…ˆå–éç©ºå€¼åˆå¹¶
        combined = df[cols].bfill(axis=1).iloc[:, 0]
        # ç§»é™¤æ‰€æœ‰é‡å¤åˆ—
        df.drop(columns=cols, inplace=True)
        # å†™å›å•åˆ—
        df[name] = combined
        merged_info[name] = len(cols)
    if merged_info:
        total_removed = sum(c - 1 for c in merged_info.values())
        print(f"  [é‡å¤åˆ—åˆå¹¶] åˆå¹¶ {len(merged_info)} ä¸ªåˆ—åï¼Œå…±ç§»é™¤ {total_removed} ä¸ªé‡å¤åˆ—")
    return merged_info

def deep_clean_nonscalars(df, skip_cols=None, verbose=True):
    """
    V3.5.8: æ·±åº¦æ¸…ç†DataFrameä¸­çš„éæ ‡é‡å•å…ƒæ ¼ã€‚
    - å¯¹äº skip_cols è·³è¿‡
    - å¦‚å•å…ƒæ ¼åŒ…å« DataFrame/Series/list/tuple/dict/ndarray â†’ ä½¿ç”¨ flatten_to_scalar å±•å¹³
    - è‹¥ä»ä¸ºéæ ‡é‡ï¼Œæœ€ç»ˆè½¬ä¸ºå­—ç¬¦ä¸²
    è¿”å›ï¼š(cleaned_by_col, offenders_after) ä¸¤ä¸ªå­—å…¸
    """
    import numpy as np
    def _is_nonscalar(v):
        return isinstance(v, (pd.DataFrame, pd.Series, list, tuple, dict, np.ndarray))
    def _flatten_to_scalar(value):
        """æœ¬åœ°æ ‡é‡å±•å¹³å™¨ï¼šæœ€å¤š10å±‚ï¼Œè¡Œä¸ºä¸ä¸»æµç¨‹ä¸­çš„flatten_to_scalarç­‰ä»·"""
        for _ in range(10):
            if isinstance(value, pd.DataFrame):
                if value.shape[0] > 0 and value.shape[1] > 0:
                    value = value.iloc[0, 0]
                    continue
                return None
            if isinstance(value, pd.Series):
                if len(value) == 0:
                    return None
                non_null = value.dropna()
                value = non_null.iloc[0] if len(non_null) > 0 else value.iloc[0]
                continue
            if isinstance(value, (list, tuple, np.ndarray)):
                if len(value) == 0:
                    return None
                seq = list(value)
                picked = None
                for item in seq:
                    if item is not None and not (isinstance(item, float) and pd.isna(item)):
                        picked = item
                        break
                value = picked if picked is not None else seq[0]
                continue
            if isinstance(value, dict):
                try:
                    value = next(iter(value.values()))
                    continue
                except StopIteration:
                    return None
            break
        if value is None or (isinstance(value, float) and pd.isna(value)):
            return None
        if isinstance(value, (str, bytes, int, float, bool)):
            return value
        if hasattr(np, 'isscalar') and np.isscalar(value):
            return value
        return str(value)

    skip = set(skip_cols or [])
    cleaned_by_col = {}
    for col in df.columns:
        if col in skip:
            continue
        s = df[col]
        # åªåœ¨å­˜åœ¨éæ ‡é‡æ—¶è¿›è¡Œå¤„ç†ï¼Œé¿å…æ€§èƒ½æŸè€—
        mask = s.map(_is_nonscalar)
        cnt = int(mask.sum())
        if cnt > 0:
            # é€å•å…ƒæ ¼å±•å¹³ â†’ éæ ‡é‡æœ€ç»ˆè½¬str
            df[col] = s.map(lambda x: (x if not _is_nonscalar(x) else x))
            df[col] = df[col].map(lambda x: _flatten_to_scalar(x) if _is_nonscalar(x) else x)
            df[col] = df[col].map(lambda x: x if not _is_nonscalar(x) else str(x))
            cleaned_by_col[col] = cnt

    # äºŒæ¬¡æ‰«æç¡®è®¤ï¼Œæ— æ®‹ç•™éæ ‡é‡ï¼›å¦‚ä»æœ‰ï¼Œå¼ºåˆ¶è½¬ä¸ºå­—ç¬¦ä¸²
    offenders_after = {}
    for col in df.columns:
        if col in skip:
            continue
        s = df[col]
        mask = s.map(lambda v: isinstance(v, (pd.DataFrame, pd.Series, list, tuple, dict, np.ndarray)))
        cnt = int(mask.sum())
        if cnt > 0:
            df[col] = s.map(lambda v: str(v) if isinstance(v, (pd.DataFrame, pd.Series, list, tuple, dict, np.ndarray)) else v)
            offenders_after[col] = cnt

    if verbose:
        if cleaned_by_col:
            total = sum(cleaned_by_col.values())
            print(f"  [æ·±åº¦æ¸…ç†] éæ ‡é‡æ¸…ç†è®¡æ•°: æ€»è®¡ {total} ä¸ªå•å…ƒ")
            for k, v in cleaned_by_col.items():
                print(f"    - {k}: {v}")
        if offenders_after:
            print(f"  [æ·±åº¦æ¸…ç†] ä»æœ‰éæ ‡é‡æ®‹ç•™(å·²å¼ºåˆ¶è½¬str): {list(offenders_after.items())}")
    return cleaned_by_col, offenders_after

def _parse_date_any(date_str: str):
    """å°è¯•è§£æ 'YYYY-MM-DD' æˆ– 'YYYYMMDD' ä¸ºdateå¯¹è±¡ï¼Œå¤±è´¥è¿”å›None"""
    from datetime import datetime
    for fmt in ("%Y-%m-%d", "%Y%m%d"):
        try:
            return datetime.strptime(date_str, fmt).date()
        except Exception:
            pass
    return None

def file_covers_date(filename: str, target_date_str: str) -> bool:
    """åˆ¤æ–­æ–‡ä»¶åæ˜¯å¦"è¦†ç›–"ç›®æ ‡æ—¥æœŸï¼ˆä»…åœ¨æŒ‡å®šæ—¥æœŸåœºæ™¯ä½¿ç”¨ï¼‰
    è§„åˆ™ï¼š
      - TT-...-YYYY-MM-DD.* ä»…å½“æ—¥æœŸ==ç›®æ ‡æ—¥æœŸ
      - *_YYYYMMDD_* ä»…å½“æ—¥æœŸ==ç›®æ ‡æ—¥æœŸ
      - *YYYYMMDD_YYYYMMDD* åŒºé—´åŒ…å«ç›®æ ‡æ—¥æœŸ
      - *YYYY-MM-DD_YYYY-MM-DD* åŒºé—´åŒ…å«ç›®æ ‡æ—¥æœŸ
    å…¶ä»–ï¼šæœªçŸ¥åˆ™è¿”å›Trueï¼ˆä¸è¿‡æ»¤ï¼‰
    """
    import re
    tgt = _parse_date_any(target_date_str)
    if not tgt:
        return True

    # å•æ—¥æœŸï¼ˆæ¨ªæ ï¼‰
    m = re.search(r"(\d{4}-\d{2}-\d{2})", filename)
    if m:
        d = _parse_date_any(m.group(1))
        return d == tgt if d else True

    # å•æ—¥æœŸï¼ˆç´§å‡‘ï¼‰
    m = re.search(r"(\d{8})", filename)
    if m:
        d = _parse_date_any(m.group(1))
        return d == tgt if d else True

    # åŒºé—´ï¼ˆç´§å‡‘ï¼‰
    m = re.search(r"(\d{8})_(\d{8})", filename)
    if m:
        d1 = _parse_date_any(m.group(1))
        d2 = _parse_date_any(m.group(2))
        if d1 and d2 and d1 <= tgt <= d2:
            return True
        return False

    # åŒºé—´ï¼ˆæ¨ªæ ï¼‰
    m = re.search(r"(\d{4}-\d{2}-\d{2}).*(\d{4}-\d{2}-\d{2})", filename)
    if m:
        d1 = _parse_date_any(m.group(1))
        d2 = _parse_date_any(m.group(2))
        if d1 and d2 and d1 <= tgt <= d2:
            return True
        return False

    return True

def list_input_files(root_dir, target_date: str = None):
    """é€’å½’æ‰«æç›®å½•ä¸‹æ‰€æœ‰CSV/Excelæ–‡ä»¶ï¼ˆV3.5.9ï¼šç™½åå•æ¨¡å¼ï¼Œä¸åšæ–‡ä»¶åæ—¥æœŸè¿‡æ»¤ï¼‰"""
    files = []
    
    print(f"[æ‰«æ] æ­£åœ¨æ‰«æç›®å½•: {root_dir}")
    print(f"[æ‰«æ] ä½¿ç”¨ç™½åå•æ¨¡å¼è¿‡æ»¤æ–‡ä»¶ï¼ˆä¸è¿‡æ»¤æ–‡ä»¶åæ—¥æœŸï¼‰")
    
    # v2.1: è·³è¿‡çš„ç›®å½•åˆ—è¡¨ï¼ˆé¿å…æ‰«æChromeå’Œcookiesç­‰æ— å…³ç›®å½•ï¼‰
    # V3.5.3: å¢å¼ºè·³è¿‡åˆ—è¡¨ï¼Œé¿å…æ‰«æè™šæ‹Ÿç¯å¢ƒ
    skip_dirs = {
        'chrome_user_data',  # Chromeæµè§ˆå™¨æ•°æ®ï¼ˆ3000+æ–‡ä»¶ï¼‰
        'cookies',           # Cookieæ–‡ä»¶
        '__pycache__',       # Pythonç¼“å­˜
        '.git',              # Gitç‰ˆæœ¬æ§åˆ¶
        'node_modules',      # Nodeä¾èµ–ï¼ˆå¦‚æœæœ‰ï¼‰
        '.venv',             # è™šæ‹Ÿç¯å¢ƒï¼ˆé‡è¦ï¼ï¼‰
        'venv',              # è™šæ‹Ÿç¯å¢ƒï¼ˆå¤‡ç”¨åï¼‰
        '.cursor',           # Cursorç¼–è¾‘å™¨ç¼“å­˜
    }
    
    # v2.1: è·³è¿‡çš„æ–‡ä»¶åæ¨¡å¼ï¼ˆå†å²è¾“å‡ºæ–‡ä»¶å’Œé…ç½®æ–‡ä»¶ï¼‰
    # V3.1: å¢å¼ºè·³è¿‡é€»è¾‘ï¼Œæ˜ç¡®æ’é™¤æ‰€æœ‰å†å²è¾“å‡ºæ–‡ä»¶
    skip_file_patterns = [
        'sites.csv',         # é…ç½®æ–‡ä»¶ï¼Œä¸æ˜¯æ•°æ®æºï¼
        'é˜ˆå€¼è¥æ”¶è¡¨',        # é…ç½®æ–‡ä»¶ï¼Œå…¬å¼å®šä¹‰è¡¨ï¼Œä¸æ˜¯æ•°æ®æºï¼
        'æ¯æ—¥æ€»ä»£æ•°æ®',      # åŒ¹é…æ‰€æœ‰"æ¯æ—¥æ€»ä»£æ•°æ®"å¼€å¤´çš„æ–‡ä»¶ï¼ˆé™¤äº†è‡ªåŠ¨ç”Ÿæˆçš„ï¼‰
        # ä¹±ç ç‰ˆæœ¬çš„æ–‡ä»¶å
        'Ã¦Â¯Ã¦â€”Â¥Ã¦â‚¬Â»Ã¤Â»Â£Ã¦â€¢Â°Ã¦Â®',
    ]
    
    scanned_count = 0
    skipped_count = 0
    
    for r, dirs, fs in os.walk(root_dir):
        # v2.1: è¿‡æ»¤æ‰ä¸éœ€è¦æ‰«æçš„å­ç›®å½•ï¼ˆç›´æ¥ä¿®æ”¹dirsåˆ—è¡¨ï¼‰
        original_dirs = len(dirs)
        dirs[:] = [d for d in dirs if d not in skip_dirs]
        skipped_dirs = original_dirs - len(dirs)
        
        for f in fs:
            scanned_count += 1
            
            # è·³è¿‡ Excel ä¸´æ—¶æ–‡ä»¶ï¼ˆä»¥ ~$ å¼€å¤´ï¼‰
            if f.startswith('~$'):
                skipped_count += 1
                continue
                
            # è·³è¿‡éšè—æ–‡ä»¶ï¼ˆä»¥ . å¼€å¤´ï¼‰
            if f.startswith('.'):
                skipped_count += 1
                continue
            
            # V3.5.8: ç™½åå•æ¨¡å¼ - ç®€æ´é«˜æ•ˆçš„è¿‡æ»¤
            # åªæ”¶é›† CSV/Excel æ–‡ä»¶
            if not f.lower().endswith((".csv", ".xlsx", ".xls")):
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ ¹ç›®å½•
            is_root_dir = (r == os.path.abspath(root_dir))
            
            # ç™½åå•æ£€æŸ¥ï¼šåªæ¥å—æœ‰æ•ˆçš„æ•°æ®æºæ–‡ä»¶
            full_path = os.path.join(r, f)
            if not is_valid_data_file(full_path, f):
                skipped_count += 1
                # åªåœ¨æ ¹ç›®å½•æ˜¾ç¤ºè¢«è¿‡æ»¤çš„æ–‡ä»¶ï¼ˆé¿å…åˆ·å±ï¼‰
                if is_root_dir:
                    print(f"  [ç™½åå•è¿‡æ»¤] {f}")
                continue
            
            # V3.5.9: ä¸å†æŒ‰æ–‡ä»¶åæ—¥æœŸè¿‡æ»¤ï¼Œä¾èµ–æ–‡ä»¶å†…å®¹çš„"æ—¥æœŸ"åˆ—
            # æ‰€æœ‰ç™½åå•æ–‡ä»¶éƒ½ä¼šè¢«è¯»å–ï¼Œåç»­æŒ‰å†…å®¹æ—¥æœŸç­›é€‰
            
            # é€šè¿‡ç™½åå•ï¼Œæ·»åŠ åˆ°æ–‡ä»¶åˆ—è¡¨
            files.append(full_path)
    
    print(f"[æ‰«æ] å®Œæˆï¼æ‰«æäº† {scanned_count} ä¸ªæ–‡ä»¶ï¼Œè·³è¿‡ {skipped_count} ä¸ªï¼Œæ‰¾åˆ° {len(files)} ä¸ªæ•°æ®æ–‡ä»¶")
    return files

def to_half_width(s):
    if not isinstance(s, str):
        s = "" if pd.isna(s) else str(s)
    return "".join(chr(ord(c)-0xFEE0) if "ï¼" <= c <= "ï¼™" else c for c in s)

def normalize_date(v):
    if pd.isna(v) or str(v).strip()=="":
        return None
    s = to_half_width(str(v)).strip()
    # å»æ‰"æ•°æ®æ±‡æ€»"
    if "æ•°æ®æ±‡æ€»" in s:
        return None
    d = pd.to_datetime(s, errors="coerce")
    if pd.isna(d):
        return None
    return d.normalize().strftime("%Y-%m-%d")

TAIL_PATTERNS = [re.compile(r"\((\d+)\)\s*$"), re.compile(r"ï¼ˆ(\d+)ï¼‰\s*$")]

def extract_agent_id_from_tail(name):
    if not isinstance(name, str):
        return None
    s = to_half_width(name).strip()
    for pat in TAIL_PATTERNS:
        m = pat.search(s)
        if m:
            return int(m.group(1))
    return None

def strip_tail_parenthesis(name):
    if not isinstance(name, str):
        return ""
    s = to_half_width(name).strip()
    for pat in TAIL_PATTERNS:
        s = pat.sub("", s).strip()
    return s

def stable_agent_id(name):
    """ä¸ºæ²¡æœ‰å°¾æ‹¬å·IDçš„åç§°ç”Ÿæˆä¸€ä¸ªç¨³å®šçš„æ­£æ•´æ•°IDï¼ˆç”¨äºä¸»é”®/èšåˆï¼‰"""
    if not isinstance(name, str) or not name:
        return None
    # ç”¨md5å‰8ä½è½¬ä¸ºæ­£æ•´æ•°ï¼Œç¡®ä¿åŒåç¨³å®š
    return int(md5(name.encode("utf-8")).hexdigest()[:8], 16)

def pick_col(df, alias_list):
    for col in alias_list:
        if col in df.columns:
            return col
    # ä¸åŒºåˆ†å¤§å°å†™å†è¯•ä¸€æ¬¡
    lower_map = {c.lower(): c for c in df.columns}
    for col in alias_list:
        lc = col.lower()
        if lc in lower_map:
            return lower_map[lc]
    return None

def parse_channel_clean(clean_name):
    # å‘½åæ ¼å¼ï¼šç›˜å£_éƒ¨é—¨_ç±»å‹ç _åª’ä»‹ç _æ–¹å¼ç _å°ç»„
    # å¢å¼ºåˆ†éš”ç¬¦å…¼å®¹ï¼šæ”¯æŒ -, ç©ºæ ¼, å¤šä¸‹åˆ’çº¿ç­‰
    s = to_half_width(str(clean_name)).strip()
    s = re.sub(r"[\-\s]+", "_", s)  # æ¨ªæ å’Œç©ºæ ¼è½¬ä¸‹åˆ’çº¿
    s = re.sub(r"_+", "_", s).strip("_")  # å¤šä¸‹åˆ’çº¿åˆå¹¶
    parts = s.split("_")
    
    platform    = parts[0] if len(parts)>0 else ""   # ç¬¬1æ®µæ˜¯"ç›˜å£"
    dept        = parts[1] if len(parts)>1 else ""
    type_code   = parts[2] if len(parts)>2 else ""
    media_code  = parts[3] if len(parts)>3 else ""
    method_code = parts[4] if len(parts)>4 else ""
    group       = parts[5] if len(parts)>5 else ""
    
    return {
        "äº§å“": "TTäº§å“",                      # äº§å“å›ºå®šä¸º"TTäº§å“"
        "ç›˜å£_token": platform,               # æš‚å­˜ï¼šåé¢èµ‹åˆ°"ç›˜å£"åˆ—
        "æ¨å¹¿éƒ¨é—¨": dept,
        "æ¨å¹¿æ–¹å¼": METHOD_MAP.get(method_code, method_code),
        "type_code": type_code,
        "type_name": TYPE_MAP.get(type_code, type_code),
        "media_code": media_code,
        "media_name": MEDIA_MAP.get(media_code, media_code),
        "method_code": method_code,
        "group_name": group
    }

def is_primary_channel(clean_name):
    """
    åˆ¤å®š"ä¸€çº§"æ¸ é“ï¼ˆç”¨äºï¼šä¸€çº§é¦–å……äººæ•°/æˆæœ¬ï¼‰ã€‚é»˜è®¤ï¼šè¿”å› Falseã€‚
    ä½ å¯ä»¥æ ¹æ®ä¸šåŠ¡æŠŠè§„åˆ™å†™åœ¨è¿™é‡Œï¼Œä¾‹å¦‚ï¼š
      - æŒ‡å®šéƒ¨é—¨=AX ä¸ºä¸€çº§
      - æˆ– æŒ‡å®šåª’ä»‹/æ–¹å¼ä¸ºä¸€çº§
    """
    return False


# ----------------------------- æ¨å¹¿æ–¹å¼åˆ¤æ–­ï¼ˆV3.0æ–°å¢ï¼‰ -----------------------------

def get_promotion_method(channel_names):
    """
    ä»æ¸ é“åç§°åˆ—è¡¨åˆ¤æ–­æ¨å¹¿æ–¹å¼
    channel_names: åŒä¸€ä»£ç†IDä¸‹æ‰€æœ‰æ¸ é“åç§°åˆ—è¡¨
    è¿”å›ï¼šæ¨å¹¿æ–¹å¼ï¼ˆå¦‚"çŸ­ä¿¡"ã€"æŠ•æ”¾"ã€"çŸ­ä¿¡+æŠ•æ”¾"ç­‰ï¼‰
    """
    if not channel_names:
        return "æŠ•æ”¾"  # é»˜è®¤
    
    # å¦‚æœæ˜¯å•ä¸ªå­—ç¬¦ä¸²ï¼Œè½¬ä¸ºåˆ—è¡¨
    if isinstance(channel_names, str):
        channel_names = [channel_names]
    
    keywords = {
        'çŸ­ä¿¡': ['dx', 'duanxin', 'çŸ­ä¿¡'],
        'æŠ•æ”¾': ['toufang', 'æŠ•æ”¾'],
        'ç½‘çº¢': ['wanghong', 'ç½‘çº¢'],
        'è‡ªæŠ•': ['zitou', 'è‡ªæŠ•'],
        'å®˜æ–¹': ['guanfang', 'å®˜æ–¹']
    }
    
    found_methods = set()
    for channel in channel_names:
        if not isinstance(channel, str):
            continue
        channel_lower = channel.lower()
        for method, keys in keywords.items():
            if any(k in channel_lower for k in keys):
                found_methods.add(method)
    
    if not found_methods:
        return 'æŠ•æ”¾'  # é»˜è®¤
    
    return '+'.join(sorted(found_methods))


# ----------------------------- LTV/ç•™å­˜ç‡æå–ï¼ˆV3.0æ–°å¢ï¼‰ -----------------------------

def extract_ltv_value(ltv_string):
    """
    ä» "11.34(110.00)" æ ¼å¼ä¸­æå– LTVå€¼ 11.34
    """
    if pd.isna(ltv_string) or str(ltv_string).strip() == "" or str(ltv_string) == "0(0)":
        return 0.0
    
    s = str(ltv_string).strip()
    # åŒ¹é…æ ¼å¼ï¼šæ•°å­—(æ•°å­—)
    match = re.match(r'([-\d.]+)\(', s)
    if match:
        try:
            return float(match.group(1))
        except:
            return 0.0
    
    # å¦‚æœæ²¡æœ‰æ‹¬å·ï¼Œç›´æ¥è½¬æ¢
    try:
        return float(s)
    except:
        return 0.0


def extract_retention_rate(ret_string):
    """
    ä» "2 (8.70%)" æˆ– "8.70%" æˆ– 0.087/8.7 ç­‰æ ¼å¼ä¸­æå–ç•™å­˜ç‡ï¼Œç»Ÿä¸€ä¸ºç™¾åˆ†æ¯”æ•°å€¼ï¼ˆå¦‚8.70ï¼‰ã€‚
    V3.6: ä¿®æ”¹ä¸ºè¿”å›ç™¾åˆ†æ¯”å½¢å¼è€Œéå°æ•°ï¼ˆ3.11è€Œä¸æ˜¯0.0311ï¼‰
    """
    if pd.isna(ret_string) or str(ret_string).strip() == "":
        return 0.0
    
    s = str(ret_string).strip()
    # 1) æ‹¬å·å†…ç™¾åˆ†æ¯”ï¼š(æ•°å­—%)
    m = re.search(r"\(([-\d.]+)%\)", s)
    if m:
        try:
            return float(m.group(1))  # V3.6: ç›´æ¥è¿”å›ç™¾åˆ†æ¯”æ•°å€¼
        except Exception:
            return 0.0
    # 2) ç›´æ¥ç™¾åˆ†æ¯”ï¼š"8.7%"
    if "%" in s:
        try:
            return float(s.replace('%', ''))  # V3.6: ç›´æ¥è¿”å›ç™¾åˆ†æ¯”æ•°å€¼
        except Exception:
            return 0.0
    # 3) çº¯æ•°å­—ï¼š0-1 è§†ä¸ºæ¯”ä¾‹éœ€è½¬æ¢ï¼Œ>1 è§†ä¸ºç™¾åˆ†æ•°
    try:
        v = float(s)
        if v <= 1.0:
            return v * 100.0  # V3.6: å°†å°æ•°è½¬ä¸ºç™¾åˆ†æ¯”
        return v
    except Exception:
        return 0.0

# ----------------------------- æ™ºèƒ½æ–‡ä»¶é€‰æ‹©ï¼ˆV3.6æ–°å¢ï¼‰ -----------------------------

def select_best_file_by_date_range(file_paths, file_type_name=""):
    """
    V3.6: ä»å¤šä¸ªæ–‡ä»¶ä¸­é€‰æ‹©æœ€ä½³æ–‡ä»¶
    ç­–ç•¥ï¼šä¼˜å…ˆé€‰æ‹©æ—¥æœŸè·¨åº¦æœ€é•¿ä¸”æœ€æ–°æ—¥æœŸæœ€è¿‘çš„æ–‡ä»¶
    
    å‚æ•°ï¼š
    - file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    - file_type_name: æ–‡ä»¶ç±»å‹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
    
    è¿”å›ï¼šæœ€ä½³æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰æœ‰æ•ˆæ–‡ä»¶åˆ™è¿”å›None
    """
    if not file_paths:
        return None
    
    if len(file_paths) == 1:
        return file_paths[0]
    
    print(f"\n  [æ™ºèƒ½æ–‡ä»¶é€‰æ‹©] {file_type_name}: å‘ç°{len(file_paths)}ä¸ªå€™é€‰æ–‡ä»¶")
    
    best_file = None
    best_score = (-1, None, -1)  # (æ—¥æœŸè·¨åº¦, æœ€æ–°æ—¥æœŸ, è¡Œæ•°)
    
    for fpath in file_paths:
        try:
            # è¯»å–æ–‡ä»¶
            df = read_any_table(fpath)
            if df.empty:
                continue
            
            # æŸ¥æ‰¾æ—¥æœŸåˆ—
            date_col = pick_col(df, ALIASES["date"])
            if not date_col:
                continue
            
            # æå–æ—¥æœŸ
            dates = pd.to_datetime(df[date_col], errors='coerce').dropna()
            if len(dates) == 0:
                continue
            
            min_date = dates.min()
            max_date = dates.max()
            date_span = (max_date - min_date).days
            row_count = len(df)
            
            fname = os.path.basename(fpath)
            print(f"    {fname}: æ—¥æœŸè·¨åº¦={date_span}å¤© ({min_date.date()}~{max_date.date()}), è¡Œæ•°={row_count}")
            
            # è¯„åˆ†: (æ—¥æœŸè·¨åº¦, æœ€æ–°æ—¥æœŸ, è¡Œæ•°)
            current_score = (date_span, max_date, row_count)
            
            if current_score > best_score:
                best_score = current_score
                best_file = fpath
        
        except Exception as e:
            print(f"    âš ï¸ è·³è¿‡æ–‡ä»¶ {os.path.basename(fpath)}: {e}")
            continue
    
    if best_file:
        print(f"  âœ“ é€‰ä¸­: {os.path.basename(best_file)} (è·¨åº¦={best_score[0]}å¤©, æœ€æ–°={best_score[1].date()}, è¡Œæ•°={best_score[2]})")
    else:
        print(f"  âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆæ–‡ä»¶")
    
    return best_file

# ----------------------------- æ–‡ä»¶åè§£æï¼ˆV3.0æ–°å¢ï¼‰ -----------------------------

def parse_filename(path):
    """
    è§£ææ–°æ ¼å¼æ–‡ä»¶åï¼šTT-{ç›˜å£}-{åœ°åŒº}-{éƒ¨é—¨}-{ç±»å‹}-{æ—¥æœŸ}.csv
    è¿”å›ï¼š{"ç›˜å£": str, "åœ°åŒº": str, "éƒ¨é—¨": str, "ç±»å‹": str, "æ—¥æœŸ": str}
    """
    filename = os.path.basename(path)
    # å»æ‰æ‰©å±•å
    name_no_ext = os.path.splitext(filename)[0]
    
    # æŒ‰"-"åˆ†å‰²
    parts = name_no_ext.split('-')
    
    result = {
        "ç›˜å£": None,
        "åœ°åŒº": None,
        "éƒ¨é—¨": None,
        "ç±»å‹": None,
        "æ—¥æœŸ": None,
        "æ±‡ç‡": EXCHANGE_RATES.get("å·´è¥¿", 6.0)  # é»˜è®¤å·´è¥¿æ±‡ç‡
    }
    
    if len(parts) >= 5:
        # TT-ç›˜å£-åœ°åŒº-éƒ¨é—¨-ç±»å‹-æ—¥æœŸæ ¼å¼
        result["ç›˜å£"] = parts[1] if len(parts) > 1 else None
        result["åœ°åŒº"] = parts[2] if len(parts) > 2 else None
        result["éƒ¨é—¨"] = parts[3] if len(parts) > 3 else None
        # ç±»å‹å’Œæ—¥æœŸå¯èƒ½ç²˜åœ¨ä¸€èµ·ï¼Œå¦‚"ä»£ç†æŠ¥è¡¨-2025-10-29"
        if len(parts) > 4:
            result["ç±»å‹"] = parts[4]
        if len(parts) > 5:
            # æ—¥æœŸéƒ¨åˆ†ï¼š2025-10-29
            date_parts = parts[5:]
            if len(date_parts) >= 3:
                result["æ—¥æœŸ"] = f"{date_parts[0]}-{date_parts[1]}-{date_parts[2]}"
        
        # æ ¹æ®åœ°åŒºè®¾ç½®æ±‡ç‡ï¼ˆä»é…ç½®è¯»å–ï¼‰
        if result["åœ°åŒº"]:
            for region, rate in EXCHANGE_RATES.items():
                if region in result["åœ°åŒº"]:
                    result["æ±‡ç‡"] = rate
                    break
    
    return result


# ----------------------------- æ–‡ä»¶åˆ†ç±» -----------------------------

def classify_file(path):
    """æ ¹æ®æ–‡ä»¶åå…³é”®å­—åˆ†ç±»æ¥æºç±»å‹ã€‚"""
    name = os.path.basename(path).lower()
    if "operation_export" in name:
        return "ops"
    if "agent_report" in name or "ä»£ç†æŠ¥è¡¨" in name:
        return "agent"
    if "platform_report" in name:
        return "platform"
    if "user_daily_export" in name:
        return "daily"
    if "first_paid_ltv" in name or "ltv" in name:
        return "fpltv"
    # v2.1 æ–°å¢ï¼šæ”¯æŒæ–°çš„æ–‡ä»¶å‘½åæ ¼å¼
    if "user_retention_first_login" in name or "é¦–å……ç”¨æˆ·ç™»å½•ç•™å­˜" in name or "ç™»å½•ç•™å­˜" in name:
        return "ret_login"
    if "user_retention_register_user" in name or "æ³¨å†Œç•™å­˜" in name:
        return "ret_register"
    if "user_retention_first_pay" in name or "é¦–å……ç”¨æˆ·ä»˜è´¹ç•™å­˜" in name or "ä»˜è´¹ç•™å­˜" in name:
        return "ret_fpay"
    # v2.1 æ–°å¢ï¼šé¦–å……ç”¨æˆ·ä¸‹æ³¨ç•™å­˜ï¼ˆæ–°ç±»å‹ï¼‰
    if "user_retention_first_play" in name or "é¦–å……ç”¨æˆ·ä¸‹æ³¨ç•™å­˜" in name or "ä¸‹æ³¨ç•™å­˜" in name:
        return "ret_play"
    if "é˜ˆå€¼è¥æ”¶è¡¨" in name or "é˜ˆå€¼" in name or "cost" in name or "ads" in name:
        return "cost"
    return "unknown"


def classify_file_smart(path):
    """
    å¢å¼ºç‰ˆæ–‡ä»¶åˆ†ç±»ï¼šå…ˆæŒ‰æ–‡ä»¶åï¼Œå†æŒ‰å†…å®¹è¯†åˆ«
    é€‚ç”¨äºçˆ¬è™«ä¸‹è½½çš„æ–‡ä»¶ï¼ˆæ–‡ä»¶åä¸åŒ…å«å…³é”®å­—ï¼‰
    """
    # 1. å…ˆç”¨åŸæœ‰çš„æ–‡ä»¶åè§„åˆ™
    typ = classify_file(path)
    if typ != "unknown":
        return typ
    
    # 2. å¦‚æœæ–‡ä»¶åè§„åˆ™å¤±è´¥ï¼Œè¯»å–æ–‡ä»¶å†…å®¹åˆ¤æ–­
    try:
        df = read_any_table(path)
        if df.empty:
            return "unknown"
        
        # è½¬æ¢åˆ—åä¸ºå­—ç¬¦ä¸²ä¾¿äºåŒ¹é…
        cols_str = " ".join([str(c) for c in df.columns])
        
        # åˆ¤æ–­è§„åˆ™ï¼š
        # ä»£ç†æŠ¥è¡¨ï¼šæœ‰æ¸ é“åˆ— + (æ³¨å†Œ|æ´»è·ƒ|å……å€¼)ç›¸å…³æŒ‡æ ‡
        has_channel = any(keyword in cols_str for keyword in ["æ¸ é“", "æ€»ä»£", "ä»£ç†", "agent", "channel", "æ€»ä»£åç§°"])
        has_register = any(keyword in cols_str for keyword in ["æ³¨å†Œ", "register", "æ–°å¢æ³¨å†Œ"])
        has_active = any(keyword in cols_str for keyword in ["æ´»è·ƒ", "active"])
        has_pay = any(keyword in cols_str for keyword in ["å……å€¼", "ä»˜è´¹", "pay", "å……å€¼äººæ•°", "å……å€¼é‡‘é¢"])
        
        if has_channel and (has_register or has_active or has_pay):
            return "agent"
        
        # é¦–å……LTVï¼šåŒ…å« FPLTV æˆ– é¦–å……LTV ç›¸å…³åˆ—
        if any(keyword in cols_str for keyword in ["FPLTV", "é¦–å……LTV", "é¦–å……ltv"]):
            return "fpltv"
        
        # å¹³å°LTVï¼šåŒ…å« ltv_d ç³»åˆ—
        if any(keyword in cols_str for keyword in ["ltv_d1", "ltv_d3", "ltv_d7", "LTV(D"]):
            return "platform"
        
        # ç•™å­˜ï¼šåŒ…å« D1/D3/D7/D14/D30 ç•™å­˜
        if any(keyword in cols_str for keyword in ["ç•™å­˜", "retention", "D1", "D3", "D7", "D14", "D30"]):
            # v2.1 ä¼˜åŒ–ï¼šæ›´ç²¾ç¡®çš„ç•™å­˜ç±»å‹è¯†åˆ«
            # ä¼˜å…ˆåŒ¹é…æ›´å…·ä½“çš„ç±»å‹ï¼Œé¿å…"é¦–å……"å…³é”®å­—å¯¼è‡´è¯¯åˆ¤
            if "ä¸‹æ³¨" in cols_str or "æŠ•æ³¨" in cols_str or "first_play" in cols_str or "play" in cols_str:
                return "ret_play"
            elif ("é¦–å……" in cols_str or "first_pay" in cols_str) and ("ä»˜è´¹" in cols_str or "å……å€¼" in cols_str):
                return "ret_fpay"
            elif "é¦–ç™»" in cols_str or "ç™»å½•" in cols_str or "first_login" in cols_str or "login" in cols_str:
                return "ret_login"
            elif "æ³¨å†Œ" in cols_str or "register" in cols_str:
                return "ret_register"
            else:
                # é»˜è®¤å½’ç±»ä¸ºæ³¨å†Œç•™å­˜
                return "ret_register"
        
        # æˆæœ¬/å¹¿å‘Šï¼šåŒ…å«æ¶ˆè€—/å±•ç¤º/ç‚¹å‡»
        if any(keyword in cols_str for keyword in ["æ¶ˆè€—", "å±•ç¤º", "ç‚¹å‡»", "spend", "cost", "impression", "click"]):
            return "cost"
        
        # æ—¥å¸¸æ•°æ®ï¼šåŒ…å«é¦–å……äººæ•°/é¦–å……é‡‘é¢
        if any(keyword in cols_str for keyword in ["é¦–å……äººæ•°", "é¦–å……é‡‘é¢", "firstpay"]):
            return "daily"
        
    except Exception as e:
        print(f"  [è¯†åˆ«è­¦å‘Š] {os.path.basename(path)}: {e}")
    
    return "unknown"

# ----------------------------- è¯»å–ä¸æ ‡å‡†åŒ– -----------------------------

def std_ops(df):
    """æ ‡å‡†åŒ–ï¼šè¿è¥æ•°æ®ï¼ˆæƒå¨æ¥æºï¼šæŠ½å–æ€»ä»£å·ï¼›å¯å¸¦æŠ•æ³¨/ä¸­å¥–/åˆ©æ¶¦ï¼‰"""
    c_date    = pick_col(df, ALIASES["date"])
    c_channel = pick_col(df, ALIASES["channel"])
    
    # å¦‚æœæ²¡æœ‰æ¸ é“åˆ—ï¼Œè¯´æ˜æ˜¯å¹³å°çº§æ±‡æ€»ï¼Œè¿”å›ç©ºï¼ˆç¨åä¼šä»å…¶ä»–æºè·å–ï¼‰
    if c_channel is None:
        print("  Note: Operation data has no channel column (platform-level aggregate), skipping...")
        return pd.DataFrame()
    
    out = pd.DataFrame()
    out["æ—¥æœŸ"] = df[c_date].map(normalize_date)
    out["æ€»ä»£åç§°"] = df[c_channel]
    out["æ€»ä»£åç§°_æ¸…æ´—"] = out["æ€»ä»£åç§°"].map(strip_tail_parenthesis)
    out["æ€»ä»£å·"] = out["æ€»ä»£åç§°"].map(extract_agent_id_from_tail)
    # è¿è¥æŒ‡æ ‡ï¼ˆè‹¥æœ‰ï¼‰
    c_bet_amt  = pick_col(df, ALIASES["bet_amt"])
    c_win_amt  = pick_col(df, ALIASES["win_amt"])
    c_bet_cnt  = pick_col(df, ALIASES["bet_cnt"])
    c_bet_user = pick_col(df, ALIASES["bet_users"])
    if c_bet_amt:  out["æŠ•æ³¨é‡‘é¢"] = pd.to_numeric(df[c_bet_amt], errors="coerce").fillna(0.0)
    if c_win_amt:  out["ä¸­å¥–é‡‘é¢"] = pd.to_numeric(df[c_win_amt], errors="coerce").fillna(0.0)
    if c_bet_cnt:  out["æŠ•æ³¨æ¬¡æ•°"] = pd.to_numeric(df[c_bet_cnt], errors="coerce").fillna(0.0)
    if c_bet_user: out["æŠ•æ³¨äººæ•°"] = pd.to_numeric(df[c_bet_user], errors="coerce").fillna(0.0)
    return out.dropna(subset=["æ—¥æœŸ","æ€»ä»£åç§°"])

def std_agent(df, name_id_map, filename=None):
    """
    æ ‡å‡†åŒ–ï¼šä»£ç†æŠ¥è¡¨ï¼ˆæ³¨å†Œ/æ´»è·ƒ/å……å€¼ï¼‰
    V3.0æ–°å¢ï¼š
    - ä¿ç•™æ¸ é“åç§°ç”¨äºæ¨å¹¿æ–¹å¼åˆ¤æ–­
    - æ”¯æŒæ±‡ç‡æ¢ç®—ï¼ˆéœ€è¦filenameæå–åœ°åŒºä¿¡æ¯ï¼‰
    - å¤„ç†"é¦–å……ä»˜è´¹é‡‘é¢"å­—æ®µï¼ˆæ˜ å°„ä¸ºå½“æ—¥é¦–å……é‡‘é¢ï¼‰
    - å¤„ç†"å……æå·®"å­—æ®µ
    """
    # V3.5.6: å¦‚æœDataFrameä¸ºç©ºï¼Œç›´æ¥è¿”å›
    if df.empty:
        return pd.DataFrame(columns=["æ—¥æœŸ","æ€»ä»£åç§°","æ€»ä»£åç§°_æ¸…æ´—","æ€»ä»£å·"])
    
    c_date    = pick_col(df, ALIASES["date"])
    c_channel = pick_col(df, ALIASES["channel"])
    
    # å¦‚æœæ²¡æœ‰æ¸ é“åˆ—ï¼Œè·³è¿‡
    if c_channel is None:
        print("  Note: Agent data has no channel column, skipping...")
        return pd.DataFrame()
    
    out = pd.DataFrame()
    out["æ—¥æœŸ"] = df[c_date].map(normalize_date)
    out["æ€»ä»£åç§°"] = df[c_channel]
    out["æ€»ä»£åç§°_æ¸…æ´—"] = out["æ€»ä»£åç§°"].map(strip_tail_parenthesis)
    
    # V3.0: ä¿ç•™æ¸ é“åç§°ï¼ˆåŸå§‹ï¼‰ï¼Œç”¨äºåç»­æ¨å¹¿æ–¹å¼åˆ¤æ–­
    # æ³¨æ„ï¼šè¿™é‡Œä¿ç•™çš„æ˜¯ 'æ¸ é“åç§°' åˆ—çš„åŸå§‹å€¼
    if 'æ¸ é“åç§°' in df.columns:
        out["æ¸ é“åç§°_åŸå§‹"] = df['æ¸ é“åç§°']
    
    # v2.1: ä¼˜å…ˆä½¿ç”¨"ä»£ç†ID"åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    c_agent_id = pick_col(df, ALIASES["agent_id"])
    if c_agent_id:
        # ç›´æ¥ä½¿ç”¨CSVæ–‡ä»¶ä¸­çš„ä»£ç†IDåˆ—
        out["æ€»ä»£å·"] = pd.to_numeric(df[c_agent_id], errors="coerce").astype("Int64")
        print(f"  [ä»£ç†æ•°æ®] ä½¿ç”¨ä»£ç†IDåˆ—: {c_agent_id}")
    elif name_id_map:
        # ä½¿ç”¨name_id_mapæ˜ å°„
        out["æ€»ä»£å·"] = out["æ€»ä»£åç§°_æ¸…æ´—"].map(name_id_map).astype("Int64")
    else:
        # ä½¿ç”¨stable_idå‡½æ•°ç”Ÿæˆæ€»ä»£å·
        from hashlib import md5
        def stable_id_func(name):
            if pd.isna(name):
                return pd.NA
            return int(md5(str(name).encode('utf-8')).hexdigest()[:8], 16)
        out["æ€»ä»£å·"] = out["æ€»ä»£åç§°_æ¸…æ´—"].map(stable_id_func).astype("Int64")
    
    # V3.0: è·å–æ±‡ç‡ï¼ˆä»æ–‡ä»¶åè§£æï¼Œå¼•ç”¨é…ç½®åŒºæ±‡ç‡ï¼‰
    # V3.2: åŒæ—¶æå–ç›˜å£ä¿¡æ¯
    # V3.6: ä»å¹³å°æ˜ å°„æ¨å¹¿éƒ¨é—¨
    exchange_rate = EXCHANGE_RATES.get("å·´è¥¿", 6.0)  # é»˜è®¤å·´è¥¿æ±‡ç‡
    file_platform = None
    file_dept = None
    if filename:
        file_info = parse_filename(filename)
        exchange_rate = file_info.get("æ±‡ç‡", EXCHANGE_RATES.get("å·´è¥¿", 6.0))
        file_platform = file_info.get("ç›˜å£", None)
        # V3.6: ä»å¹³å°æ˜ å°„æ¨å¹¿éƒ¨é—¨
        if file_platform:
            file_dept = PLATFORM_TO_DEPT.get(file_platform, file_platform)
        print(f"  [æ–‡ä»¶è§£æ] ç›˜å£: {file_platform}, æ¨å¹¿éƒ¨é—¨: {file_dept}, åœ°åŒº: {file_info.get('åœ°åŒº', 'æœªçŸ¥')}, æ±‡ç‡: {exchange_rate}")
    
    # åŸºç¡€æŒ‡æ ‡
    c_reg   = pick_col(df, ALIASES["register"])
    c_act   = pick_col(df, ALIASES["active"])
    c_pu    = pick_col(df, ALIASES["pay_users"])
    c_pa    = pick_col(df, ALIASES["pay_amount"])
    c_fpu   = pick_col(df, ALIASES["firstpay_u"])
    c_fpa   = pick_col(df, ALIASES["firstpay_a"])
    c_wd    = pick_col(df, ALIASES["withdraw"])
    
    # V3.0: æ”¯æŒ"å……æå·®"å­—æ®µ
    c_deposit_withdraw_diff = pick_col(df, ["å……æå·®", "å……å€¼æç°å·®"])
    
    if c_reg: out["æ³¨å†Œäººæ•°"] = pd.to_numeric(df[c_reg], errors="coerce").fillna(0).astype("Int64")
    if c_act: out["æ´»è·ƒäººæ•°"] = pd.to_numeric(df[c_act], errors="coerce").fillna(0).astype("Int64")
    if c_pu:  out["å……å€¼äººæ•°"] = pd.to_numeric(df[c_pu], errors="coerce").fillna(0).astype("Int64")
    
    # V3.0: é‡‘é¢å­—æ®µéœ€è¦æ±‡ç‡æ¢ç®—
    if c_pa:  
        out["å……å€¼é‡‘é¢"] = pd.to_numeric(df[c_pa], errors="coerce").fillna(0.0) / exchange_rate
    if c_fpu: 
        out["é¦–å……äººæ•°"] = pd.to_numeric(df[c_fpu], errors="coerce").fillna(0).astype("Int64")
    if c_fpa: 
        out["å½“æ—¥é¦–å……é‡‘é¢"] = pd.to_numeric(df[c_fpa], errors="coerce").fillna(0.0) / exchange_rate
    if c_wd:  
        out["æç°é‡‘é¢"] = pd.to_numeric(df[c_wd], errors="coerce").fillna(0.0) / exchange_rate
    if c_deposit_withdraw_diff:
        out["å……æå·®"] = pd.to_numeric(df[c_deposit_withdraw_diff], errors="coerce").fillna(0.0) / exchange_rate
    
    # V3.2: æ·»åŠ ç›˜å£ä¿¡æ¯ï¼ˆä»æ–‡ä»¶åæå–ï¼‰
    if file_platform:
        out["ç›˜å£"] = file_platform
    
    # V3.6: æ·»åŠ æ¨å¹¿éƒ¨é—¨ä¿¡æ¯ï¼ˆä»å¹³å°æ˜ å°„ï¼‰
    if file_dept:
        out["æ¨å¹¿éƒ¨é—¨"] = file_dept
    
    return out.dropna(subset=["æ—¥æœŸ","æ€»ä»£åç§°_æ¸…æ´—"])

def std_platform(df):
    """æ ‡å‡†åŒ–ï¼šå¹³å°æŠ¥è¡¨ï¼ˆå¹³å°LTVï¼›å¹³å°/ç›˜å£çº§ï¼‰"""
    # V3.5.6: å¦‚æœDataFrameä¸ºç©ºï¼Œç›´æ¥è¿”å›
    if df.empty:
        return pd.DataFrame(columns=["æ—¥æœŸ","ç›˜å£"])
    
    c_date = pick_col(df, ALIASES["date"])
    c_plat = pick_col(df, ALIASES["platform"])
    out = pd.DataFrame()
    out["æ—¥æœŸ"] = df[c_date].map(normalize_date)
    out["ç›˜å£"] = df[c_plat]
    # LTV
    for std_key, aliases in [("ltv_D1","ltv_d1"),("ltv_D3","ltv_d3"),("ltv_D7","ltv_d7"),("ltv_D14","ltv_d14"),("ltv_D30","ltv_d30")]:
        col = pick_col(df, ALIASES[aliases])
        if col:
            out[std_key] = pd.to_numeric(df[col], errors="coerce").fillna(0.0)
    return out.dropna(subset=["æ—¥æœŸ","ç›˜å£"])

def std_daily(df, name_id_map):
    """æ ‡å‡†åŒ–ï¼šç”¨æˆ·æ—¥å¸¸ï¼ˆé¦–å……/æ´»è·ƒå……å€¼ï¼‰"""
    # V3.5.6: å¦‚æœDataFrameä¸ºç©ºï¼Œç›´æ¥è¿”å›
    if df.empty:
        return pd.DataFrame(columns=["æ—¥æœŸ"])
    
    c_date    = pick_col(df, ALIASES["date"])
    c_channel = pick_col(df, ALIASES["channel"])
    out = pd.DataFrame()
    out["æ—¥æœŸ"] = df[c_date].map(normalize_date)
    if c_channel:
        out["æ€»ä»£åç§°"] = df[c_channel]
        out["æ€»ä»£åç§°_æ¸…æ´—"] = out["æ€»ä»£åç§°"].map(strip_tail_parenthesis)
        # v2.1: ä¼˜å…ˆä½¿ç”¨"ä»£ç†ID"åˆ—
        c_agent_id = pick_col(df, ALIASES["agent_id"])
        if c_agent_id:
            out["æ€»ä»£å·"] = pd.to_numeric(df[c_agent_id], errors="coerce").astype("Int64")
        else:
            out["æ€»ä»£å·"] = out["æ€»ä»£åç§°_æ¸…æ´—"].map(name_id_map).astype("Int64")
    # æŒ‡æ ‡
    for k, std_name in [("firstpay_u","é¦–å……äººæ•°"),("firstpay_a","å½“æ—¥é¦–å……é‡‘é¢"),("pay_active_u","æ´»è·ƒå……å€¼äººæ•°")]:
        col = pick_col(df, ALIASES[k])
        if col:
            if "äººæ•°" in std_name:
                out[std_name] = pd.to_numeric(df[col], errors="coerce").fillna(0).astype("Int64")
            else:
                out[std_name] = pd.to_numeric(df[col], errors="coerce").fillna(0.0)
    return out.dropna(subset=["æ—¥æœŸ"])

def extract_primary_firstpay(df):
    """
    V3.0æ–°å¢ï¼šä»ç•™å­˜æ•°æ®ä¸­æå–ä¸€çº§é¦–å……äººæ•°
    ä¸€çº§é¦–å……äººæ•° = è£‚å˜ç±»å‹ä¸º"parent"çš„é¦–å……äººæ•°
    """
    c_date = pick_col(df, ALIASES["date"])
    c_agent_id = pick_col(df, ALIASES["agent_id"])
    c_channel = pick_col(df, ALIASES["channel"])
    
    if c_date is None:
        return pd.DataFrame()
    
    # è¿‡æ»¤è£‚å˜ç±»å‹
    if "è£‚å˜ç±»å‹" in df.columns:
        target_type = "å…¨éƒ¨" if PRIMARY_FIRSTPAY_SOURCE == "all" else "parent"
        df = df[df["è£‚å˜ç±»å‹"] == target_type].copy()
        print(f"  [ä¸€çº§é¦–å……] è¿‡æ»¤è£‚å˜ç±»å‹={target_type}ï¼Œå‰©ä½™ {len(df)} è¡Œ")
    else:
        return pd.DataFrame()
    
    out = pd.DataFrame()
    out["æ—¥æœŸ"] = df[c_date].map(normalize_date)
    
    # æå–ä»£ç†ID
    if c_agent_id:
        out["æ€»ä»£å·"] = pd.to_numeric(df[c_agent_id], errors="coerce").astype("Int64")
    elif c_channel:
        out["æ€»ä»£åç§°"] = df[c_channel]
        out["æ€»ä»£åç§°_æ¸…æ´—"] = out["æ€»ä»£åç§°"].map(strip_tail_parenthesis)
        out["æ€»ä»£å·"] = out["æ€»ä»£åç§°"].map(extract_agent_id_from_tail)
    
    # æå–é¦–å……äººæ•°ä½œä¸ºä¸€çº§é¦–å……äººæ•°
    c_firstpay = pick_col(df, ["é¦–å……äººæ•°", "firstpay_u"])
    if c_firstpay:
        out["ä¸€çº§é¦–å……äººæ•°"] = pd.to_numeric(df[c_firstpay], errors="coerce").fillna(0).astype("Int64")
    
    return out.dropna(subset=["æ—¥æœŸ"])


def std_retention(df, which="login", filename=None):
    """
    æ ‡å‡†åŒ–ï¼šç•™å­˜ï¼ˆå¹³å°/ç›˜å£çº§ä¸ºä¸»ï¼›å¦‚æœæ¥æºæœ‰æ¸ é“åˆ—ï¼Œå¯æ‰©å±•ï¼‰
    V3.0æ–°å¢ï¼š
    - è¿‡æ»¤è£‚å˜ç±»å‹="å…¨éƒ¨"
    - è§£æç•™å­˜ç‡ç™¾åˆ†æ¯”æ ¼å¼ï¼š"2 (8.70%)" â†’ 0.087
    - è¿”å›ç•™å­˜æ•°æ®ï¼ŒåŒæ—¶å¯æå–ä¸€çº§é¦–å……äººæ•°ï¼ˆéœ€å•ç‹¬è°ƒç”¨extract_primary_firstpayï¼‰
    V3.2æ–°å¢ï¼š
    - ä»æ–‡ä»¶åæå–ç›˜å£ä¿¡æ¯
    """
    # V3.5.6: å¦‚æœDataFrameä¸ºç©ºï¼Œç›´æ¥è¿”å›
    if df.empty:
        return pd.DataFrame(columns=["æ—¥æœŸ"])
    
    c_date = pick_col(df, ALIASES["date"])
    c_plat = pick_col(df, ALIASES["platform"])
    c_channel = pick_col(df, ALIASES["channel"])
    
    # V3.2: ä»æ–‡ä»¶åæå–ç›˜å£
    file_platform = None
    if filename:
        file_info = parse_filename(filename)
        file_platform = file_info.get("ç›˜å£", None)
    
    # v2.1: å¦‚æœæ‰¾ä¸åˆ°æ—¥æœŸåˆ—ï¼Œè¿”å›ç©ºDataFrame
    if c_date is None:
        print(f"  [è­¦å‘Š] ç•™å­˜æ•°æ®ç¼ºå°‘æ—¥æœŸåˆ—ï¼Œåˆ—å: {df.columns.tolist()[:10]}")
        return pd.DataFrame()
    
    # V3.0: è¿‡æ»¤è£‚å˜ç±»å‹="å…¨éƒ¨"
    if "è£‚å˜ç±»å‹" in df.columns:
        df = df[df["è£‚å˜ç±»å‹"] == "å…¨éƒ¨"].copy()
        print(f"  [ç•™å­˜æ•°æ®] è¿‡æ»¤è£‚å˜ç±»å‹=å…¨éƒ¨ï¼Œå‰©ä½™ {len(df)} è¡Œ")
    
    out = pd.DataFrame()
    out["æ—¥æœŸ"] = df[c_date].map(normalize_date)
    
    # If has channel column, use it
    if c_channel:
        out["æ€»ä»£åç§°"] = df[c_channel]
        out["æ€»ä»£åç§°_æ¸…æ´—"] = out["æ€»ä»£åç§°"].map(strip_tail_parenthesis)
        # v2.1: ä¼˜å…ˆä½¿ç”¨"ä»£ç†ID"åˆ—
        c_agent_id = pick_col(df, ALIASES["agent_id"])
        if c_agent_id:
            out["æ€»ä»£å·"] = pd.to_numeric(df[c_agent_id], errors="coerce").astype("Int64")
        else:
            out["æ€»ä»£å·"] = out["æ€»ä»£åç§°"].map(extract_agent_id_from_tail)
    
    # V3.2: ä¼˜å…ˆä½¿ç”¨æ–‡ä»¶åä¸­çš„ç›˜å£ä¿¡æ¯
    if file_platform:
        out["ç›˜å£"] = file_platform
    elif c_plat:
        out["ç›˜å£"] = df[c_plat]
    
    # V3.0: è§£æç•™å­˜ç‡ï¼ˆæ”¯æŒç‰¹æ®Šæ ¼å¼ï¼‰
    # æŸ¥æ‰¾ç•™å­˜ç‡åˆ—ï¼š2æ—¥ç•™å­˜ã€3æ—¥ç•™å­˜ã€7æ—¥ç•™å­˜ã€15æ—¥ç•™å­˜ã€30æ—¥ç•™å­˜
    retention_mapping = {
        1: ["2æ—¥ç•™å­˜", "D1", "ç•™å­˜ç‡(D1)"],
        3: ["3æ—¥ç•™å­˜", "D3", "ç•™å­˜ç‡(D3)"],
        7: ["7æ—¥ç•™å­˜", "D7", "ç•™å­˜ç‡(D7)"],
        15: ["15æ—¥ç•™å­˜", "D15", "ç•™å­˜ç‡(D15)"],
        30: ["30æ—¥ç•™å­˜", "D30", "ç•™å­˜ç‡(D30)"]
    }
    
    for days, col_names in retention_mapping.items():
        col = pick_col(df, col_names)
        if col:
            # ä½¿ç”¨extract_retention_rateå‡½æ•°è§£æ
            out[f"{which}_D{days}"] = df[col].map(extract_retention_rate)
    
    return out.dropna(subset=["æ—¥æœŸ"])

def std_fpltv(df, filename=None):
    """
    æ ‡å‡†åŒ–ï¼šé¦–å……LTVï¼ˆæŒ‰ä»£ç†IDï¼‰
    V3.0æ–°å¢ï¼š
    - æ”¯æŒæ–°æ ¼å¼ï¼š"ç¬¬1å¤©"ã€"ç¬¬2å¤©"...åˆ—
    - è§£æLTVç‰¹æ®Šæ ¼å¼ï¼š"11.34(110.00)" â†’ 11.34
    V3.2æ–°å¢ï¼š
    - ä»æ–‡ä»¶åæå–ç›˜å£ä¿¡æ¯
    """
    # V3.5.6: å¦‚æœDataFrameä¸ºç©ºï¼Œç›´æ¥è¿”å›
    if df.empty:
        return pd.DataFrame(columns=["æ—¥æœŸ"])
    
    c_date = pick_col(df, ALIASES["date"])
    c_plat = pick_col(df, ALIASES["platform"])
    c_agent_id = pick_col(df, ALIASES["agent_id"])
    c_channel = pick_col(df, ALIASES["channel"])
    
    # V3.2: ä»æ–‡ä»¶åæå–ç›˜å£
    file_platform = None
    if filename:
        file_info = parse_filename(filename)
        file_platform = file_info.get("ç›˜å£", None)
    
    out = pd.DataFrame()
    out["æ—¥æœŸ"] = df[c_date].map(normalize_date)
    
    # V3.0: æ”¯æŒä»£ç†IDç»´åº¦
    if c_agent_id:
        out["æ€»ä»£å·"] = pd.to_numeric(df[c_agent_id], errors="coerce").astype("Int64")
    
    if c_channel:
        out["æ€»ä»£åç§°"] = df[c_channel]
        out["æ€»ä»£åç§°_æ¸…æ´—"] = out["æ€»ä»£åç§°"].map(strip_tail_parenthesis)
    
    # V3.2: ä¼˜å…ˆä½¿ç”¨æ–‡ä»¶åä¸­çš„ç›˜å£ä¿¡æ¯
    if file_platform:
        out["ç›˜å£"] = file_platform
    elif c_plat:
        out["ç›˜å£"] = df[c_plat]
    
    # V3.0: è§£ææ–°æ ¼å¼LTVæ•°æ®ï¼ˆ"ç¬¬Xå¤©"åˆ—ï¼‰
    # V3.5.10: æ”¯æŒ"é¦–å……/è€ƒæ ¸N"æ ¼å¼ï¼ˆç”¨æˆ·å®é™…æ–‡ä»¶æ ¼å¼ï¼‰
    # V3.5.11: æ·»åŠ D14æ”¯æŒï¼Œå…¼å®¹ä¸åŒç«™ç‚¹å£å¾„
    day_columns_mapping = {
        "FPLTV_D1": ["é¦–å……", "è€ƒæ ¸1", "ç¬¬1å¤©", "FPLTV_D1", "fpltv_d1", "D1"],
        "FPLTV_D2": ["è€ƒæ ¸2", "ç¬¬2å¤©", "FPLTV_D2", "fpltv_d2", "D2"],
        "FPLTV_D3": ["è€ƒæ ¸3", "ç¬¬3å¤©", "FPLTV_D3", "fpltv_d3", "D3"],
        "FPLTV_D7": ["è€ƒæ ¸7", "ç¬¬7å¤©", "FPLTV_D7", "fpltv_d7", "D7"],
        "FPLTV_D14": ["è€ƒæ ¸14", "ç¬¬14å¤©", "FPLTV_D14", "fpltv_d14", "D14"],
        "FPLTV_D15": ["è€ƒæ ¸15", "ç¬¬15å¤©", "FPLTV_D15", "fpltv_d15", "D15"],
        "FPLTV_D30": ["è€ƒæ ¸30", "ç¬¬30å¤©", "FPLTV_D30", "fpltv_d30", "D30"]
    }
    
    for key, col_names in day_columns_mapping.items():
        col = pick_col(df, col_names)
        if col:
            # ä½¿ç”¨extract_ltv_valueå‡½æ•°è§£æ
            out[key] = df[col].map(extract_ltv_value)
    
    return out.dropna(subset=["æ—¥æœŸ"])

def std_cost(df):
    """æ ‡å‡†åŒ–ï¼šæˆæœ¬/å¹¿å‘Šæ•°æ®ï¼ˆæ¶ˆè€—/å±•ç¤º/ç‚¹å‡»/æç°ç­‰ï¼‰ï¼Œå¯ä»é˜ˆå€¼è¥æ”¶è¡¨æˆ–å¹¿å‘ŠCSVè¯»å–ã€‚å¹³å°æˆ–æ¸ é“çº§éƒ½æ”¯æŒã€‚"""
    c_date = pick_col(df, ALIASES["date"])
    
    # V3.5.4: å¦‚æœæ²¡æœ‰æ—¥æœŸåˆ—ï¼Œè¯´æ˜æ˜¯é…ç½®æ–‡ä»¶ï¼ˆå¦‚é˜ˆå€¼è¥æ”¶è¡¨ï¼‰ï¼Œè¿”å›ç©ºDataFrame
    if c_date is None:
        print(f"  [è·³è¿‡] Costæ•°æ®ç¼ºå°‘æ—¥æœŸåˆ—ï¼Œå¯èƒ½æ˜¯é…ç½®æ–‡ä»¶")
        return pd.DataFrame()
    
    out = pd.DataFrame()
    out["æ—¥æœŸ"] = df[c_date].map(normalize_date)
    c_channel = pick_col(df, ALIASES["channel"])
    c_plat    = pick_col(df, ALIASES["platform"])
    if c_channel:
        out["æ€»ä»£åç§°"] = df[c_channel]
        out["æ€»ä»£åç§°_æ¸…æ´—"] = out["æ€»ä»£åç§°"].map(strip_tail_parenthesis)
    if c_plat:
        out["ç›˜å£"] = df[c_plat]
    for k, std in [("spend","æ¶ˆè€—"),("impr","å±•ç¤º"),("click","ç‚¹å‡»"),("withdraw","æç°é‡‘é¢")]:
        col = pick_col(df, ALIASES[k])
        if col:
            out[std] = pd.to_numeric(df[col], errors="coerce").fillna(0.0)
    return out.dropna(subset=["æ—¥æœŸ"])

# ----------------------------- ä¸»æµç¨‹ -----------------------------

def main(input_dir, output_path, target_date=None):
    """
    ç”Ÿæˆæ¯æ—¥æ€»ä»£æ•°æ®æŠ¥è¡¨
    
    å‚æ•°:
        input_dir: è¾“å…¥ç›®å½•è·¯å¾„
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        target_date: ç›®æ ‡æ—¥æœŸï¼ˆå­—ç¬¦ä¸²ï¼Œå¦‚ "2025-10-27"ï¼‰
                    - None æˆ– "latest": è‡ªåŠ¨ä½¿ç”¨æœ€æ–°æ—¥æœŸ
                    - å…·ä½“æ—¥æœŸ: åªå¤„ç†è¯¥æ—¥æœŸçš„æ•°æ®
    """
    # V3.5.3: ä¼˜åŒ–æ‰«æç­–ç•¥ - åªæ‰«ædownloadsç›®å½•å’Œæ ¹ç›®å½•å•ç‹¬æ–‡ä»¶
    files = []
    downloads_path = os.path.join(input_dir, "downloads")
    if os.path.exists(downloads_path):
        print(f"[æ‰«æ] downloadsç›®å½•...")
        files.extend(list_input_files(downloads_path, target_date))
    
    # æ‰«ææ ¹ç›®å½•çš„å•ç‹¬æ–‡ä»¶ï¼ˆå¦‚è¿è¥æŠ¥è¡¨.xlsxï¼‰
    # V3.5.8: æ ¹ç›®å½•ä¹Ÿå¿…é¡»èµ°ç™½åå•æ£€æŸ¥
    # V3.5.9: ä¸å†æŒ‰æ–‡ä»¶åæ—¥æœŸè¿‡æ»¤
    print(f"[æ‰«æ] æ ¹ç›®å½•å•ç‹¬æ–‡ä»¶...")
    try:
        for f in os.listdir(input_dir):
            full_path = os.path.join(input_dir, f)
            if os.path.isfile(full_path) and f.lower().endswith(('.csv', '.xlsx', '.xls')):
                # V3.5.8: å¿…é¡»é€šè¿‡ç™½åå•æ£€æŸ¥
                if is_valid_data_file(full_path, f):
                    # V3.5.9: ä¸å†æŒ‰æ–‡ä»¶åæ—¥æœŸè¿‡æ»¤ï¼Œä¾èµ–æ–‡ä»¶å†…å®¹
                    files.append(full_path)
                else:
                    print(f"  [ç™½åå•è¿‡æ»¤] {f}")
    except Exception as e:
        print(f"  [è­¦å‘Š] æ‰«ææ ¹ç›®å½•æ—¶å‡ºé”™: {e}")
    
    if not files:
        print(f"[ERROR] è¾“å…¥ç›®å½• {input_dir} ä¸‹æ²¡æœ‰ä»»ä½•æ•°æ®æ–‡ä»¶ï¼Œè·³è¿‡ç”Ÿæˆã€‚")
        return
    print(f"[INFO] æ‰¾åˆ° {len(files)} ä¸ªæ•°æ®æ–‡ä»¶")
    
    # 1) è¯»å–è¿è¥æ•°æ®ï¼Œæ„å»º name_id_map
    ops_list = []
    for p in files:
        if classify_file_smart(p)=="ops":
            df = read_any_table(p)
            result = std_ops(df)
            if not result.empty:
                ops_list.append(result)
    
    if not ops_list:
        print("  Warning: No operation data with channel column found, cannot extract agent IDs from names.")
        name_id_map = {}
    else:
        ops = pd.concat(ops_list, ignore_index=True).dropna(subset=["æ—¥æœŸ","æ€»ä»£åç§°"])
        # name -> idï¼ˆå‡ºç°æ¬¡æ•°æœ€å¤šçš„IDï¼‰
        tmp = ops[["æ€»ä»£åç§°","æ€»ä»£åç§°_æ¸…æ´—","æ€»ä»£å·"]].dropna(subset=["æ€»ä»£åç§°_æ¸…æ´—","æ€»ä»£å·"])
        if tmp.empty:
            name_id_map = {}
        else:
            mode_id = (tmp.groupby("æ€»ä»£åç§°_æ¸…æ´—")["æ€»ä»£å·"]
                         .agg(lambda s: s.value_counts().index[0]))
            name_id_map = mode_id.to_dict()

    # 2) V3.6: å…ˆå¯¹æ–‡ä»¶è¿›è¡Œåˆ†ç±»å’Œåˆ†ç»„
    print("\n[åˆ†ç±»æ–‡ä»¶]")
    file_groups = {
        "agent": [], "platform": [], "daily": [], "ops": [],
        "ret_login": [], "ret_register": [], "ret_fpay": [], "ret_play": [],
        "fpltv": [], "cost": [], "unknown": []
    }
    
    for p in files:
        typ = classify_file_smart(p)
        if typ in file_groups:
            file_groups[typ].append(p)
        else:
            file_groups["unknown"].append(p)
    
    # æ‰“å°åˆ†ç±»ç»Ÿè®¡
    for typ, paths in file_groups.items():
        if paths:
            print(f"  {typ}: {len(paths)} ä¸ªæ–‡ä»¶")
    
    # V3.6: å¯¹éœ€è¦æ™ºèƒ½é€‰æ‹©çš„ç±»å‹åº”ç”¨æ™ºèƒ½æ–‡ä»¶é€‰æ‹©
    print("\n[æ™ºèƒ½æ–‡ä»¶é€‰æ‹©] å¼€å§‹ç­›é€‰æœ€ä½³æ–‡ä»¶...")
    smart_select_types = ["ret_login", "ret_register", "ret_fpay", "ret_play", "fpltv"]
    selected_files = {}
    
    for typ in smart_select_types:
        if file_groups[typ]:
            best_file = select_best_file_by_date_range(file_groups[typ], typ)
            if best_file:
                selected_files[typ] = [best_file]  # åªä½¿ç”¨æœ€ä½³æ–‡ä»¶
            else:
                selected_files[typ] = []
        else:
            selected_files[typ] = []
    
    # éæ™ºèƒ½é€‰æ‹©ç±»å‹ä¿æŒåŸæœ‰é€»è¾‘ï¼ˆå¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼‰
    for typ in ["agent", "platform", "daily", "cost"]:
        selected_files[typ] = file_groups[typ]
    
    # 3) å¤„ç†é€‰ä¸­çš„æ–‡ä»¶
    print("\n[å¤„ç†æ–‡ä»¶]")
    agent_list, platform_list, daily_list = [], [], []
    ret_login_list, ret_register_list, ret_fpay_list, ret_play_list = [], [], [], []
    fpltv_list, cost_list = [], []
    primary_firstpay_list = []  # V3.0: ä¸€çº§é¦–å……äººæ•°åˆ—è¡¨

    # åˆå¹¶æ‰€æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶
    all_selected = []
    for typ, paths in selected_files.items():
        all_selected.extend([(p, typ) for p in paths])
    
    for idx, (p, typ) in enumerate(all_selected, 1):
        print(f"  å¤„ç†æ–‡ä»¶ {idx}/{len(all_selected)}: {os.path.basename(p)} ({typ})")
        df = read_any_table(p)
        result = None
        
        if typ=="agent":
            # V3.0: ä¼ å…¥filenameå‚æ•°ä»¥æ”¯æŒæ±‡ç‡æ¢ç®—
            result = std_agent(df, name_id_map, filename=p)
            if not result.empty:
                agent_list.append(result)
        elif typ=="platform":
            result = std_platform(df)
            if not result.empty:
                platform_list.append(result)
        elif typ=="daily":
            result = std_daily(df, name_id_map)
            if not result.empty:
                daily_list.append(result)
        elif typ=="ret_login":
            # V3.2: ä¼ å…¥filenameå‚æ•°ä»¥æå–ç›˜å£
            result = std_retention(df, which="ret_login", filename=p)
            if not result.empty:
                ret_login_list.append(result)
            # V3.0: åŒæ—¶æå–ä¸€çº§é¦–å……äººæ•°ï¼ˆä»ç™»å½•ç•™å­˜æ–‡ä»¶ï¼‰
            primary_fp = extract_primary_firstpay(df)
            if not primary_fp.empty:
                primary_firstpay_list.append(primary_fp)
        elif typ=="ret_register":
            # V3.2: ä¼ å…¥filenameå‚æ•°ä»¥æå–ç›˜å£
            result = std_retention(df, which="ret_register", filename=p)
            if not result.empty:
                ret_register_list.append(result)
        elif typ=="ret_fpay":
            # V3.2: ä¼ å…¥filenameå‚æ•°ä»¥æå–ç›˜å£
            result = std_retention(df, which="ret_fpay", filename=p)
            if not result.empty:
                ret_fpay_list.append(result)
        elif typ=="ret_play":  # v2.1: æ–°å¢ä¸‹æ³¨ç•™å­˜å¤„ç†
            # V3.2: ä¼ å…¥filenameå‚æ•°ä»¥æå–ç›˜å£
            result = std_retention(df, which="ret_play", filename=p)
            if not result.empty:
                ret_play_list.append(result)
        elif typ=="fpltv":
            # V3.2: ä¼ å…¥filenameå‚æ•°ä»¥æå–ç›˜å£
            result = std_fpltv(df, filename=p)
            if not result.empty:
                fpltv_list.append(result)
        elif typ=="cost":
            result = std_cost(df)
            if not result.empty:
                cost_list.append(result)
        else:
            # å…¶ä»–æœªçŸ¥æ–‡ä»¶å¿½ç•¥
            pass

    # åˆå¹¶å„æ¥æºï¼ˆV3.5.8ï¼šä¿®å¤pandas FutureWarningï¼‰
    agent = pd.concat(agent_list, ignore_index=True) if agent_list else pd.DataFrame(columns=["æ—¥æœŸ","æ€»ä»£åç§°","æ€»ä»£åç§°_æ¸…æ´—","æ€»ä»£å·"])
    daily  = pd.concat(daily_list, ignore_index=True) if daily_list else pd.DataFrame(columns=["æ—¥æœŸ"])
    platform = pd.concat(platform_list, ignore_index=True) if platform_list else pd.DataFrame(columns=["æ—¥æœŸ","ç›˜å£"])
    
    # V3.5.8: ä¿®å¤FutureWarning - ä½¿ç”¨is_meaningful_dfè¿‡æ»¤
    if ret_login_list:
        non_empty = [df for df in ret_login_list if is_meaningful_df(df, required_cols=["æ—¥æœŸ","ç›˜å£","æ€»ä»£å·"])]
        ret_login = pd.concat(non_empty, ignore_index=True) if non_empty else pd.DataFrame(columns=["æ—¥æœŸ"])
    else:
        ret_login = pd.DataFrame(columns=["æ—¥æœŸ"])
    
    ret_register = pd.concat(ret_register_list, ignore_index=True) if ret_register_list else pd.DataFrame(columns=["æ—¥æœŸ"])
    
    # V3.5.8: ä¿®å¤FutureWarning - ä½¿ç”¨is_meaningful_dfè¿‡æ»¤
    if ret_fpay_list:
        non_empty = [df for df in ret_fpay_list if is_meaningful_df(df, required_cols=["æ—¥æœŸ","ç›˜å£","æ€»ä»£å·"])]
        ret_fpay = pd.concat(non_empty, ignore_index=True) if non_empty else pd.DataFrame(columns=["æ—¥æœŸ"])
    else:
        ret_fpay = pd.DataFrame(columns=["æ—¥æœŸ"])
    
    # V3.5.8: ä¿®å¤FutureWarning - ä½¿ç”¨is_meaningful_dfè¿‡æ»¤ï¼ˆret_playï¼‰
    if ret_play_list:
        non_empty = [df for df in ret_play_list if is_meaningful_df(df, required_cols=["æ—¥æœŸ","ç›˜å£","æ€»ä»£å·"]) ]
        ret_play = pd.concat(non_empty, ignore_index=True) if non_empty else pd.DataFrame(columns=["æ—¥æœŸ"])
    else:
        ret_play = pd.DataFrame(columns=["æ—¥æœŸ"])  # v2.1: æ–°å¢ä¸‹æ³¨ç•™å­˜
    fpltv = pd.concat(fpltv_list, ignore_index=True) if fpltv_list else pd.DataFrame(columns=["æ—¥æœŸ"])
    cost  = pd.concat(cost_list, ignore_index=True) if cost_list else pd.DataFrame(columns=["æ—¥æœŸ"])
    
    # V3.5.8: ä¿®å¤FutureWarning - ä¸¥æ ¼è¿‡æ»¤primary_firstpay
    if primary_firstpay_list:
        # æ›´ä¸¥æ ¼ï¼šè¿‡æ»¤æ‰å«ä»»ä½•å…¨NAåˆ—çš„DataFrame
        def is_clean_df(df):
            if not is_meaningful_df(df, required_cols=["æ—¥æœŸ","æ€»ä»£å·"]):
                return False
            # é¢å¤–æ£€æŸ¥ï¼šç§»é™¤å…¨NAçš„åˆ—åä»æœ‰æ•°æ®
            cleaned = df.dropna(axis=1, how='all')
            return not cleaned.empty and cleaned.shape[1] > 0
        non_empty = [df for df in primary_firstpay_list if is_clean_df(df)]
        print(f"  [primary_firstpayè¿‡æ»¤] {len(primary_firstpay_list)} -> {len(non_empty)} ä¸ªæœ‰æ•ˆDataFrame")
        primary_firstpay = pd.concat(non_empty, ignore_index=True) if non_empty else pd.DataFrame(columns=["æ—¥æœŸ","æ€»ä»£å·"])
    else:
        primary_firstpay = pd.DataFrame(columns=["æ—¥æœŸ","æ€»ä»£å·"])
    if not primary_firstpay.empty and all(c in primary_firstpay.columns for c in ["æ—¥æœŸ", "æ€»ä»£å·", "ä¸€çº§é¦–å……äººæ•°"]):
        # æŒ‰æ—¥æœŸ+æ€»ä»£å·å»é‡ï¼Œå¯¹ä¸€çº§é¦–å……äººæ•°æ±‚å’Œ
        primary_firstpay = primary_firstpay.groupby(["æ—¥æœŸ", "æ€»ä»£å·"], as_index=False)["ä¸€çº§é¦–å……äººæ•°"].sum()
        print(f"  Primary firstpay: {len(primary_firstpay)} rows after dedup")

    # ğŸ”§ ä¿®å¤ï¼šå…ˆå¯¹agentæ•°æ®å»é‡ï¼Œå†æ„å»ºbase_keys
    # V3.0: åŒæ—¶è®¡ç®—æ¨å¹¿æ–¹å¼ï¼ˆä»æ¸ é“åç§°åˆ¤æ–­ï¼‰
    # V3.3: ä¿®å¤å…³é”®åˆ—è¢«é”™è¯¯èšåˆçš„é—®é¢˜
    if not agent.empty and all(c in agent.columns for c in ["æ—¥æœŸ","æ€»ä»£å·"]):
        print(f"  Agent raw data: {len(agent)} rows")
        # æŒ‰åˆå¹¶é”®(æ—¥æœŸ+æ€»ä»£å·)å»é‡ï¼Œå¯¹æ•°å€¼åˆ—æ±‚å’Œï¼Œå¯¹æ–‡æœ¬åˆ—å–ç¬¬ä¸€ä¸ª
        merge_keys_dedup = ["æ—¥æœŸ", "æ€»ä»£å·"]
        text_cols = ["æ€»ä»£åç§°", "æ€»ä»£åç§°_æ¸…æ´—"]
        
        # V3.3: æ˜ç¡®ä¿æŠ¤å…³é”®æ–‡æœ¬åˆ—ï¼ˆç›˜å£ã€æ¨å¹¿æ–¹å¼ç­‰ï¼‰
        protected_text_cols = []
        if "ç›˜å£" in agent.columns:
            protected_text_cols.append("ç›˜å£")
        if "æ¨å¹¿æ–¹å¼" in agent.columns:
            protected_text_cols.append("æ¨å¹¿æ–¹å¼")
        
        all_text_cols = text_cols + protected_text_cols
        
        # V3.0: ç‰¹æ®Šå¤„ç†æ¸ é“åç§°åˆ—ï¼ˆç”¨äºæ¨å¹¿æ–¹å¼åˆ¤æ–­ï¼‰
        channel_col_name = "æ¸ é“åç§°_åŸå§‹"
        has_channel_names = channel_col_name in agent.columns
        
        # V3.3: æ•°å€¼åˆ—ï¼šæ’é™¤merge_keysã€æ‰€æœ‰æ–‡æœ¬åˆ—ã€ç‰¹æ®Šåˆ—
        data_cols = [c for c in agent.columns 
                     if c not in merge_keys_dedup + all_text_cols + ([channel_col_name] if has_channel_names else [])]
        
        print(f"  [Agentå»é‡] æ–‡æœ¬åˆ—: {all_text_cols}")
        print(f"  [Agentå»é‡] æ•°å€¼åˆ—: {data_cols[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ª
        
        # åˆ†åˆ«å¤„ç†æ–‡æœ¬åˆ—å’Œæ•°å€¼åˆ—
        agg_dict = {}
        for col in all_text_cols:
            if col in agent.columns:
                agg_dict[col] = 'first'  # å–ç¬¬ä¸€ä¸ªå€¼
        
        # V3.0: æ”¶é›†æ‰€æœ‰æ¸ é“åç§°ç”¨äºæ¨å¹¿æ–¹å¼åˆ¤æ–­
        if has_channel_names:
            agg_dict[channel_col_name] = lambda x: list(x)  # æ”¶é›†æ‰€æœ‰æ¸ é“åç§°
        
        for col in data_cols:
            if col in agent.columns:
                agg_dict[col] = 'sum'  # æ•°å€¼æ±‚å’Œ
        
        if agg_dict:
            agent = agent.groupby(merge_keys_dedup, as_index=False).agg(agg_dict)
            print(f"  Agent after dedup by date+id: {len(agent)} rows")
            
            # V3.3: æ£€æŸ¥å»é‡åçš„å…³é”®åˆ—
            if "ç›˜å£" in agent.columns:
                non_empty = agent["ç›˜å£"].notna().sum()
                print(f"  [Agentå»é‡å] ç›˜å£åˆ—: {non_empty} / {len(agent)} éç©º")
                if non_empty > 0:
                    print(f"  [Agentå»é‡å] ç›˜å£æ ·æœ¬: {agent['ç›˜å£'].head(3).tolist()}")
            
            # V3.0: è®¡ç®—æ¨å¹¿æ–¹å¼
            if has_channel_names and channel_col_name in agent.columns:
                agent["æ¨å¹¿æ–¹å¼"] = agent[channel_col_name].map(get_promotion_method)
                print(f"  Added æ¨å¹¿æ–¹å¼ column based on channel names")
                # åˆ é™¤ä¸´æ—¶çš„æ¸ é“åç§°åˆ—ï¼ˆä¸éœ€è¦ä¿ç•™åˆ°æœ€ç»ˆè¾“å‡ºï¼‰
                agent = agent.drop(columns=[channel_col_name])
    
    # ğŸ”§ ä¿®å¤ï¼šå¯¹dailyæ•°æ®ä¹Ÿå»é‡
    if not daily.empty and all(c in daily.columns for c in ["æ—¥æœŸ","æ€»ä»£å·"]):
        print(f"  Daily raw data: {len(daily)} rows")
        merge_keys_dedup = ["æ—¥æœŸ", "æ€»ä»£å·"]
        text_cols = ["æ€»ä»£åç§°", "æ€»ä»£åç§°_æ¸…æ´—"]
        data_cols = [c for c in daily.columns if c not in merge_keys_dedup + text_cols]
        
        agg_dict = {}
        for col in text_cols:
            if col in daily.columns:
                agg_dict[col] = 'first'
        for col in data_cols:
            if col in daily.columns:
                agg_dict[col] = 'sum'
        
        if agg_dict:
            daily = daily.groupby(merge_keys_dedup, as_index=False).agg(agg_dict)
            print(f"  Daily after dedup by date+id: {len(daily)} rows")

    # ğŸ”§ ä¿®å¤ï¼šåªå¤„ç†æŒ‡å®šæ—¥æœŸçš„æ•°æ®ï¼ˆæ¯æ—¥æŠ¥è¡¨åŠŸèƒ½ï¼‰
    print("\n[3.5] ç­›é€‰ç›®æ ‡æ—¥æœŸçš„æ•°æ®ï¼ˆæ¯æ—¥æŠ¥è¡¨ï¼‰...")
    all_dates = set()
    # V3.1: æ·»åŠ æ—¥æœŸåˆ†å¸ƒç»Ÿè®¡
    date_stats = {}
    
    # V3.0: æ·»åŠ primary_firstpayåˆ°æ—¥æœŸç­›é€‰åˆ—è¡¨
    for name, df in [("agent", agent), ("daily", daily), ("platform", platform), 
                     ("ret_login", ret_login), ("ret_register", ret_register), 
                     ("ret_fpay", ret_fpay), ("ret_play", ret_play), 
                     ("fpltv", fpltv), ("cost", cost), ("primary_firstpay", primary_firstpay)]:
        if not df.empty and "æ—¥æœŸ" in df.columns:
            dates = df["æ—¥æœŸ"].dropna().unique()
            all_dates.update(dates)
            date_stats[name] = {str(d): len(df[df["æ—¥æœŸ"] == d]) for d in dates}
    
    # V3.1: è¾“å‡ºæ—¥æœŸåˆ†å¸ƒç»Ÿè®¡
    if date_stats:
        print("\n  [æ—¥æœŸåˆ†å¸ƒç»Ÿè®¡]")
        for name, dates in date_stats.items():
            if dates:
                print(f"    {name}: {dates}")
    
    if all_dates:
        # ç¡®å®šè¦ä½¿ç”¨çš„æ—¥æœŸ
        if target_date and target_date != "latest":
            # ä½¿ç”¨æŒ‡å®šçš„æ—¥æœŸ
            selected_date = target_date
            if selected_date in all_dates:
                print(f"  ä½¿ç”¨æŒ‡å®šæ—¥æœŸ: {selected_date}")
            else:
                print(f"  âš ï¸ æŒ‡å®šæ—¥æœŸ {selected_date} ä¸å­˜åœ¨äºæ•°æ®ä¸­")
                print(f"  å¯ç”¨æ—¥æœŸ: {sorted(all_dates)}")
                print(f"  å°†ä½¿ç”¨æœ€æ–°æ—¥æœŸä»£æ›¿")
                selected_date = max(all_dates)
                print(f"  å®é™…ä½¿ç”¨æ—¥æœŸ: {selected_date}")
        else:
            # é»˜è®¤æ—¥æœŸç­–ç•¥ï¼ˆV3.5.8ï¼‰ï¼šä¼˜å…ˆä½¿ç”¨Agentçš„æœ€å¤§æ—¥æœŸ
            if not agent.empty and "æ—¥æœŸ" in agent.columns:
                selected_date = agent["æ—¥æœŸ"].dropna().max()
                print(f"  ä½¿ç”¨æŒ‡å®šæ—¥æœŸ: {selected_date} (æ¥æº: agent)")
            else:
                selected_date = max(all_dates)
                print(f"  è‡ªåŠ¨ä½¿ç”¨æœ€æ–°æ—¥æœŸ: {selected_date}")
        
        # è¿‡æ»¤æ‰€æœ‰æ•°æ®æºï¼Œåªä¿ç•™é€‰å®šæ—¥æœŸ
        if not agent.empty and "æ—¥æœŸ" in agent.columns:
            before = len(agent)
            agent = agent[agent["æ—¥æœŸ"] == selected_date].copy()
            print(f"  Agent: {before} -> {len(agent)} rows")
            
            # V3.1: å¦‚æœagentæ•°æ®è¢«è¿‡æ»¤ä¸º0ï¼Œå‘å‡ºè­¦å‘Š
            if len(agent) == 0 and before > 0:
                print(f"  âš ï¸ è­¦å‘Šï¼šAgentæ•°æ®è¢«å®Œå…¨è¿‡æ»¤ï¼")
                print(f"     - æŒ‡å®šæ—¥æœŸ: {selected_date}")
                print(f"     - Agentæ•°æ®ä¸­çš„æ—¥æœŸ: {date_stats.get('agent', {})}")
                print(f"     - å»ºè®®ï¼šæ£€æŸ¥æ•°æ®æºæ–‡ä»¶æ—¥æœŸæ˜¯å¦æ­£ç¡®ï¼Œæˆ–è°ƒæ•´TARGET_DATEé…ç½®")
        
        if not daily.empty and "æ—¥æœŸ" in daily.columns:
            before = len(daily)
            daily = daily[daily["æ—¥æœŸ"] == selected_date].copy()
            print(f"  Daily: {before} -> {len(daily)} rows")
        
        if not platform.empty and "æ—¥æœŸ" in platform.columns:
            before = len(platform)
            platform = platform[platform["æ—¥æœŸ"] == selected_date].copy()
            print(f"  Platform: {before} -> {len(platform)} rows")
        
        # V3.5.9: ç•™å­˜æ•°æ®ä¿ç•™å†å²è®°å½•ï¼ˆç±»ä¼¼LTVå¤„ç†ï¼‰
        ret_login_for_base = pd.DataFrame()
        ret_login_historical = pd.DataFrame()
        if not ret_login.empty and "æ—¥æœŸ" in ret_login.columns:
            before = len(ret_login)
            ret_login["æ—¥æœŸ_dt"] = pd.to_datetime(ret_login["æ—¥æœŸ"])
            selected_dt = pd.to_datetime(selected_date)
            ret_login_for_base = ret_login[ret_login["æ—¥æœŸ_dt"] == selected_dt].copy()
            ret_login_historical = ret_login[ret_login["æ—¥æœŸ_dt"] <= selected_dt].copy()
            ret_login_for_base.drop(columns=["æ—¥æœŸ_dt"], inplace=True, errors='ignore')
            ret_login_historical.drop(columns=["æ—¥æœŸ_dt"], inplace=True, errors='ignore')
            print(f"  Retention(login)åŸºåº§: {before} -> {len(ret_login_for_base)} rows (ä»… {selected_date})")
            print(f"  Retention(login)å†å²: {before} -> {len(ret_login_historical)} rows (ç”¨äºç•™å­˜ç‡å€’æ¨)")
        
        ret_register_for_base = pd.DataFrame()
        ret_register_historical = pd.DataFrame()
        if not ret_register.empty and "æ—¥æœŸ" in ret_register.columns:
            before = len(ret_register)
            ret_register["æ—¥æœŸ_dt"] = pd.to_datetime(ret_register["æ—¥æœŸ"])
            selected_dt = pd.to_datetime(selected_date)
            ret_register_for_base = ret_register[ret_register["æ—¥æœŸ_dt"] == selected_dt].copy()
            ret_register_historical = ret_register[ret_register["æ—¥æœŸ_dt"] <= selected_dt].copy()
            ret_register_for_base.drop(columns=["æ—¥æœŸ_dt"], inplace=True, errors='ignore')
            ret_register_historical.drop(columns=["æ—¥æœŸ_dt"], inplace=True, errors='ignore')
            print(f"  Retention(register)åŸºåº§: {before} -> {len(ret_register_for_base)} rows (ä»… {selected_date})")
            print(f"  Retention(register)å†å²: {before} -> {len(ret_register_historical)} rows (ç”¨äºç•™å­˜ç‡å€’æ¨)")
        
        ret_fpay_for_base = pd.DataFrame()
        ret_fpay_historical = pd.DataFrame()
        if not ret_fpay.empty and "æ—¥æœŸ" in ret_fpay.columns:
            before = len(ret_fpay)
            ret_fpay["æ—¥æœŸ_dt"] = pd.to_datetime(ret_fpay["æ—¥æœŸ"])
            selected_dt = pd.to_datetime(selected_date)
            ret_fpay_for_base = ret_fpay[ret_fpay["æ—¥æœŸ_dt"] == selected_dt].copy()
            ret_fpay_historical = ret_fpay[ret_fpay["æ—¥æœŸ_dt"] <= selected_dt].copy()
            ret_fpay_for_base.drop(columns=["æ—¥æœŸ_dt"], inplace=True, errors='ignore')
            ret_fpay_historical.drop(columns=["æ—¥æœŸ_dt"], inplace=True, errors='ignore')
            print(f"  Retention(fpay)åŸºåº§: {before} -> {len(ret_fpay_for_base)} rows (ä»… {selected_date})")
            print(f"  Retention(fpay)å†å²: {before} -> {len(ret_fpay_historical)} rows (ç”¨äºç•™å­˜ç‡å€’æ¨)")
        
        ret_play_for_base = pd.DataFrame()
        ret_play_historical = pd.DataFrame()
        if not ret_play.empty and "æ—¥æœŸ" in ret_play.columns:
            before = len(ret_play)
            ret_play["æ—¥æœŸ_dt"] = pd.to_datetime(ret_play["æ—¥æœŸ"])
            selected_dt = pd.to_datetime(selected_date)
            ret_play_for_base = ret_play[ret_play["æ—¥æœŸ_dt"] == selected_dt].copy()
            ret_play_historical = ret_play[ret_play["æ—¥æœŸ_dt"] <= selected_dt].copy()
            ret_play_for_base.drop(columns=["æ—¥æœŸ_dt"], inplace=True, errors='ignore')
            ret_play_historical.drop(columns=["æ—¥æœŸ_dt"], inplace=True, errors='ignore')
            print(f"  Retention(play)åŸºåº§: {before} -> {len(ret_play_for_base)} rows (ä»… {selected_date})")
            print(f"  Retention(play)å†å²: {before} -> {len(ret_play_historical)} rows (ç”¨äºç•™å­˜ç‡å€’æ¨)")
        
        # V3.5.14: FPLTVåˆ†ç¦»ï¼šåŸºåº§ç”¨å½“æ—¥æ•°æ®ï¼ŒLTVå€’æ¨ç”¨å†å²æ•°æ®
        fpltv_for_base = pd.DataFrame()
        fpltv_historical = pd.DataFrame()
        if not fpltv.empty and "æ—¥æœŸ" in fpltv.columns:
            before = len(fpltv)
            fpltv["æ—¥æœŸ_dt"] = pd.to_datetime(fpltv["æ—¥æœŸ"])
            selected_dt = pd.to_datetime(selected_date)
            # åªä¿ç•™ç›®æ ‡æ—¥æœŸçš„æ•°æ®ç”¨äºåŸºåº§æ„å»º
            fpltv_for_base = fpltv[fpltv["æ—¥æœŸ_dt"] == selected_dt].copy()
            # ä¿ç•™å®Œæ•´å†å²æ•°æ®ç”¨äºLTVå€’æ¨å–å€¼
            fpltv_historical = fpltv[fpltv["æ—¥æœŸ_dt"] <= selected_dt].copy()
            fpltv_for_base.drop(columns=["æ—¥æœŸ_dt"], inplace=True, errors='ignore')
            fpltv_historical.drop(columns=["æ—¥æœŸ_dt"], inplace=True, errors='ignore')
            print(f"  FPLTVåŸºåº§: {before} -> {len(fpltv_for_base)} rows (ä»… {selected_date})")
            print(f"  FPLTVå†å²: {before} -> {len(fpltv_historical)} rows (ç”¨äºLTVå€’æ¨)")
        
        if not cost.empty and "æ—¥æœŸ" in cost.columns:
            before = len(cost)
            cost = cost[cost["æ—¥æœŸ"] == selected_date].copy()
            print(f"  Cost: {before} -> {len(cost)} rows")
        
        # V3.0: è¿‡æ»¤ä¸€çº§é¦–å……äººæ•°æ•°æ®
        if not primary_firstpay.empty and "æ—¥æœŸ" in primary_firstpay.columns:
            before = len(primary_firstpay)
            primary_firstpay = primary_firstpay[primary_firstpay["æ—¥æœŸ"] == selected_date].copy()
            print(f"  Primary firstpay: {before} -> {len(primary_firstpay)} rows")
        
        print(f"  âœ“ å·²è¿‡æ»¤ä¸ºç›®æ ‡æ—¥æœŸ {selected_date} çš„æ•°æ®")
    else:
        print("  âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆæ—¥æœŸï¼Œå°†å¤„ç†æ‰€æœ‰æ•°æ®")

    # V3.5.5: FPLTVå’Œç•™å­˜æ•°æ®å»é‡ï¼ˆåœ¨æ—¥æœŸç­›é€‰åï¼‰
    print("\n[3.6] FPLTV/ç•™å­˜æ•°æ®å»é‡...")
    
    # V3.5.14: å¯¹åŸºåº§ç”¨çš„fpltv_for_baseå’Œå†å²ç”¨çš„fpltv_historicalåˆ†åˆ«å»é‡
    if not fpltv_for_base.empty and "æ€»ä»£å·" in fpltv_for_base.columns:
        before = len(fpltv_for_base)
        dedup_keys = ["æ—¥æœŸ"]
        if "ç›˜å£" in fpltv_for_base.columns:
            dedup_keys.append("ç›˜å£")
        if "æ€»ä»£å·" in fpltv_for_base.columns:
            dedup_keys.append("æ€»ä»£å·")
        
        # æŒ‰å»é‡é”®åˆ†ç»„ï¼Œæ•°å€¼åˆ—å–å¹³å‡å€¼
        numeric_cols = [c for c in fpltv_for_base.columns if c not in dedup_keys and pd.api.types.is_numeric_dtype(fpltv_for_base[c])]
        if numeric_cols:
            fpltv_for_base = fpltv_for_base.groupby(dedup_keys, as_index=False)[numeric_cols].mean()
        print(f"  FPLTVåŸºåº§å»é‡: {before} -> {len(fpltv_for_base)} è¡Œï¼ˆæŒ‰{dedup_keys}ï¼‰")
    
    # å¯¹å†å²æ•°æ®ä¹Ÿå»é‡ï¼ˆç”¨äºLTVå€’æ¨ï¼‰
    if not fpltv_historical.empty and "æ€»ä»£å·" in fpltv_historical.columns:
        before = len(fpltv_historical)
        dedup_keys = ["æ—¥æœŸ"]
        if "ç›˜å£" in fpltv_historical.columns:
            dedup_keys.append("ç›˜å£")
        if "æ€»ä»£å·" in fpltv_historical.columns:
            dedup_keys.append("æ€»ä»£å·")
        
        numeric_cols = [c for c in fpltv_historical.columns if c not in dedup_keys and pd.api.types.is_numeric_dtype(fpltv_historical[c])]
        if numeric_cols:
            fpltv_historical = fpltv_historical.groupby(dedup_keys, as_index=False)[numeric_cols].mean()
        print(f"  FPLTVå†å²å»é‡: {before} -> {len(fpltv_historical)} è¡Œï¼ˆæŒ‰{dedup_keys}ï¼‰")
    
    # V3.5.9: ç•™å­˜æ•°æ®å»é‡ï¼ˆå¯¹_for_baseç‰ˆæœ¬å»é‡ï¼‰
    for name, df_ret in [("ret_login", ret_login_for_base), ("ret_fpay", ret_fpay_for_base), 
                          ("ret_play", ret_play_for_base), ("ret_register", ret_register_for_base)]:
        if not df_ret.empty and "æ€»ä»£å·" in df_ret.columns:
            before = len(df_ret)
            dedup_keys = ["æ—¥æœŸ"]
            if "ç›˜å£" in df_ret.columns:
                dedup_keys.append("ç›˜å£")
            if "æ€»ä»£å·" in df_ret.columns:
                dedup_keys.append("æ€»ä»£å·")
            
            # æŒ‰å»é‡é”®åˆ†ç»„ï¼Œæ•°å€¼åˆ—å–å¹³å‡å€¼
            numeric_cols = [c for c in df_ret.columns if c not in dedup_keys and pd.api.types.is_numeric_dtype(df_ret[c])]
            if numeric_cols:
                df_ret = df_ret.groupby(dedup_keys, as_index=False)[numeric_cols].mean()
                
                # æ›´æ–°åŸDataFrameï¼ˆæ›´æ–°_for_baseç‰ˆæœ¬ï¼‰
                if name == "ret_login":
                    ret_login_for_base = df_ret
                elif name == "ret_fpay":
                    ret_fpay_for_base = df_ret
                elif name == "ret_play":
                    ret_play_for_base = df_ret
                elif name == "ret_register":
                    ret_register_for_base = df_ret
                
                print(f"  {name}åŸºåº§å»é‡: {before} -> {len(df_ret)} è¡Œï¼ˆæŒ‰{dedup_keys}ï¼‰")

    # 3) ç”Ÿæˆä¸»é”®å¹¶å¹¿æ’­å¹³å°çº§
    # V3.5.12: ä»¥agentä¸ºä¸»åŸºåº§ï¼Œä»å…¶ä»–æ¥æºè¡¥å……ç¼ºå¤±ç»„åˆ
    print("\n[V3.5.12] æ„å»ºåŸºåº§ï¼šä¸»åŸºåº§(agent) + è¡¥å……(ret/fpltv)...")
    
    # 1) agentä½œä¸ºä¸»åŸºåº§ï¼ˆä¿ç•™å®Œæ•´ä¿¡æ¯ï¼‰
    if not agent.empty:
        base_cols = ["æ—¥æœŸ","æ€»ä»£åç§°","æ€»ä»£åç§°_æ¸…æ´—","æ€»ä»£å·"]
        if "ç›˜å£" in agent.columns:
            base_cols.append("ç›˜å£")
        if "æ¨å¹¿éƒ¨é—¨" in agent.columns:  # V3.6: ä¿ç•™æ¨å¹¿éƒ¨é—¨
            base_cols.append("æ¨å¹¿éƒ¨é—¨")
        if "æ¨å¹¿æ–¹å¼" in agent.columns:
            base_cols.append("æ¨å¹¿æ–¹å¼")
        
        base_keys = agent[base_cols].drop_duplicates()
        agent_platforms = base_keys["ç›˜å£"].nunique() if "ç›˜å£" in base_keys.columns else 0
        agent_agents = base_keys["æ€»ä»£å·"].nunique() if "æ€»ä»£å·" in base_keys.columns else 0
        print(f"  ä¸»åŸºåº§(agent): {len(base_keys)}è¡Œ / {agent_platforms}ç›˜å£ / {agent_agents}æ€»ä»£")
    else:
        base_keys = pd.DataFrame(columns=["æ—¥æœŸ","æ€»ä»£åç§°","æ€»ä»£åç§°_æ¸…æ´—","æ€»ä»£å·","ç›˜å£","æ¨å¹¿éƒ¨é—¨","æ¨å¹¿æ–¹å¼"])
        print(f"  è­¦å‘Šï¼šagentä¸ºç©ºï¼ŒåŸºåº§ä»å…¶ä»–æ¥æºæ„å»º")
    
    # 2) ä»å…¶ä»–æ¥æºè¡¥å……"åœ¨å®ƒä»¬æœ‰ä½†agentæ— "çš„ç»„åˆ
    # é¢„å…ˆå‡†å¤‡agentçš„æ€»ä»£åç§°æ˜ å°„è¡¨ï¼ˆè·¨æ—¥æœŸ/ç›˜å£ï¼ŒæŒ‰æ€»ä»£å·æŸ¥æ‰¾åç§°ï¼‰
    if not agent.empty and "æ€»ä»£å·" in agent.columns:
        agent_name_map = agent[["æ€»ä»£å·","æ€»ä»£åç§°","æ€»ä»£åç§°_æ¸…æ´—"]].dropna(subset=["æ€»ä»£å·"]).drop_duplicates("æ€»ä»£å·")
    else:
        agent_name_map = pd.DataFrame(columns=["æ€»ä»£å·","æ€»ä»£åç§°","æ€»ä»£åç§°_æ¸…æ´—"])
    
    # V3.5.14: åŸºåº§è¡¥å……ä½¿ç”¨fpltv_for_baseå’Œret_*_for_baseï¼ˆåªå«ç›®æ ‡æ—¥æœŸï¼‰
    supplement_sources = [(ret_login_for_base, 'ret_login'), (ret_play_for_base, 'ret_play'), 
                          (ret_fpay_for_base, 'ret_fpay'), (fpltv_for_base, 'fpltv')]
    
    total_supplemented = 0
    for src, name in supplement_sources:
        if not isinstance(src, pd.DataFrame) or src.empty:
            continue
        if "ç›˜å£" not in src.columns or "æ€»ä»£å·" not in src.columns or "æ—¥æœŸ" not in src.columns:
            continue
        
        # è¯¥æ¥æºçš„[æ—¥æœŸ,ç›˜å£,æ€»ä»£å·]ç»„åˆ
        src_keys = src[["æ—¥æœŸ","ç›˜å£","æ€»ä»£å·"]].drop_duplicates()
        
        # anti-joinï¼šæ‰¾å‡º"åœ¨srcä½†ä¸åœ¨base_keys"çš„ç»„åˆ
        if not base_keys.empty and "ç›˜å£" in base_keys.columns and "æ€»ä»£å·" in base_keys.columns:
            merged = src_keys.merge(
                base_keys[["æ—¥æœŸ","ç›˜å£","æ€»ä»£å·"]], 
                on=["æ—¥æœŸ","ç›˜å£","æ€»ä»£å·"], 
                how="left", 
                indicator=True
            )
            new_keys = merged[merged["_merge"] == "left_only"][["æ—¥æœŸ","ç›˜å£","æ€»ä»£å·"]].copy()
        else:
            new_keys = src_keys.copy()
        
        if not new_keys.empty:
            # ä»agentå›å¡«æ€»ä»£åç§°ï¼ˆæŒ‰æ€»ä»£å·åŒ¹é…ï¼‰
            if not agent_name_map.empty:
                new_keys = new_keys.merge(agent_name_map, on="æ€»ä»£å·", how="left")
            else:
                new_keys["æ€»ä»£åç§°"] = None
                new_keys["æ€»ä»£åç§°_æ¸…æ´—"] = None
            
            new_keys["æ¨å¹¿æ–¹å¼"] = None  # åç»­ä»æ¸ é“åç§°æ¨æ–­
            
            # æ·»åŠ åˆ°åŸºåº§
            base_keys = pd.concat([base_keys, new_keys], ignore_index=True)
            total_supplemented += len(new_keys)
            print(f"  è¡¥å……({name}): +{len(new_keys)}è¡Œ")
    
    if total_supplemented > 0:
        final_platforms = base_keys["ç›˜å£"].nunique() if "ç›˜å£" in base_keys.columns else 0
        final_agents = base_keys["æ€»ä»£å·"].nunique() if "æ€»ä»£å·" in base_keys.columns else 0
        print(f"\n  âœ“ åŸºåº§æœ€ç»ˆ: {len(base_keys)}è¡Œ / {final_platforms}ç›˜å£ / {final_agents}æ€»ä»£ï¼ˆè¡¥å……+{total_supplemented}ï¼‰")
    
    # Check if base_keys is empty
    print(f"  Base keys shape: {base_keys.shape if isinstance(base_keys, pd.DataFrame) else 'None'}")
    if base_keys.empty:
        print("  Error: No valid data found. Cannot generate report.")
        return
    
    # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
    for col in ["æ€»ä»£åç§°_æ¸…æ´—", "æ€»ä»£åç§°", "æ€»ä»£å·", "æ¨å¹¿æ–¹å¼", "ç›˜å£"]:
        if col not in base_keys.columns:
            base_keys[col] = None
    
    print(f"  Base keys: {len(base_keys)} unique combinations")
    
    # è§£æå‘½åä¸²ï¼ˆäº§å“/éƒ¨é—¨/æ–¹å¼ç­‰ï¼‰
    parse_df = base_keys.copy()
    # V3.5.11: åªå¯¹éç©ºçš„æ€»ä»£åç§°è¿›è¡Œè§£æ
    # V3.6: å¦‚æœæ¨å¹¿éƒ¨é—¨å·²å­˜åœ¨ï¼ˆä»å¹³å°æ˜ å°„ï¼‰ï¼Œä¿ç•™ï¼›å¦åˆ™ä»æ€»ä»£åç§°è§£æï¼ˆå…œåº•ï¼‰
    def safe_parse(name):
        if pd.isna(name) or name is None:
            return pd.Series({"äº§å“": None, "æ¨å¹¿éƒ¨é—¨_parsed": None})
        result = parse_channel_clean(name)
        return pd.Series({"äº§å“": result["äº§å“"], "æ¨å¹¿éƒ¨é—¨_parsed": result["æ¨å¹¿éƒ¨é—¨"]})
    
    parsed = parse_df["æ€»ä»£åç§°_æ¸…æ´—"].apply(safe_parse)
    parse_df = pd.concat([parse_df, parsed], axis=1)
    
    # V3.6: æ¨å¹¿éƒ¨é—¨ä¼˜å…ˆçº§ï¼š1) ä»agent/å¹³å°æ˜ å°„çš„ 2) ä»æ€»ä»£åç§°è§£æçš„ï¼ˆå…œåº•ï¼‰
    if "æ¨å¹¿éƒ¨é—¨" not in parse_df.columns:
        parse_df["æ¨å¹¿éƒ¨é—¨"] = parse_df["æ¨å¹¿éƒ¨é—¨_parsed"]
    else:
        # æœ‰æ¨å¹¿éƒ¨é—¨ä½†ä¸ºç©ºçš„è¡Œï¼Œç”¨è§£æç»“æœå¡«å……
        parse_df["æ¨å¹¿éƒ¨é—¨"] = parse_df["æ¨å¹¿éƒ¨é—¨"].fillna(parse_df["æ¨å¹¿éƒ¨é—¨_parsed"])
    
    # åˆ é™¤ä¸´æ—¶åˆ—
    if "æ¨å¹¿éƒ¨é—¨_parsed" in parse_df.columns:
        parse_df = parse_df.drop(columns=["æ¨å¹¿éƒ¨é—¨_parsed"])
    
    print(f"  After parsing: {len(parse_df)} rows, æ¨å¹¿éƒ¨é—¨éç©º: {parse_df['æ¨å¹¿éƒ¨é—¨'].notna().sum()}")
    
    # V3.5.12: ç¡®ä¿äº§å“å­—æ®µå¡«å……"TTäº§å“"
    if "äº§å“" in parse_df.columns:
        before_fill = parse_df["äº§å“"].notna().sum()
        parse_df["äº§å“"] = parse_df["äº§å“"].fillna("TTäº§å“")
        after_fill = parse_df["äº§å“"].notna().sum()
        print(f"  äº§å“å­—æ®µå¡«å……: {before_fill} -> {after_fill} éç©ºï¼ˆé»˜è®¤'TTäº§å“'ï¼‰")
    else:
        parse_df["äº§å“"] = "TTäº§å“"
        print(f"  äº§å“å­—æ®µåˆ›å»º: ç»Ÿä¸€ä¸º'TTäº§å“'")

    # V3.2: ç›˜å£å­—æ®µä¼˜å…ˆçº§è°ƒæ•´
    # V3.3: ç®€åŒ–é€»è¾‘ï¼Œä¿®å¤åˆ—å†²çªé—®é¢˜
    # V3.4: å¦‚æœbase_keyså·²åŒ…å«ç›˜å£ï¼Œç›´æ¥ä½¿ç”¨
    
    print("\n[ç›˜å£èµ‹å€¼]")
    
    # å‡†å¤‡ä¸€ä¸ªå¹³å°æ¥æºæŒ‰[æ—¥æœŸ,ç›˜å£]çš„å”¯ä¸€é”®ï¼Œæ–¹ä¾¿åç»­å¹¿æ’­
    plat_keys = platform[["æ—¥æœŸ","ç›˜å£"]].dropna().drop_duplicates() if not platform.empty else pd.DataFrame(columns=["æ—¥æœŸ","ç›˜å£"])
    
    # V3.4: ä¼˜å…ˆçº§1 - å¦‚æœbase_keyså·²ç»åŒ…å«ç›˜å£ï¼ˆæ¥è‡ªagentï¼Œä»æ–‡ä»¶åæå–ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
    if "ç›˜å£" in parse_df.columns:
        non_empty = parse_df["ç›˜å£"].notna().sum()
        print(f"  âœ“ Base_keysä¸­å·²æœ‰ç›˜å£: {non_empty} / {len(parse_df)} éç©º")
        if non_empty > 0:
            print(f"  ç›˜å£æ ·æœ¬: {parse_df['ç›˜å£'].dropna().head(5).tolist()}")
    else:
        # ä¼˜å…ˆçº§2: ä»parse_channel_cleanè·å–
        if "ç›˜å£_token" in parse_df.columns:
            parse_df["ç›˜å£"] = parse_df["ç›˜å£_token"]
            print(f"  ä»æ¸ é“åç§°è§£æ: {parse_df['ç›˜å£'].notna().sum()} æ¡")
        else:
            parse_df["ç›˜å£"] = None
        
        # ä¼˜å…ˆçº§3: ç”¨"éƒ¨é—¨â†’ç›˜å£"æ˜ å°„ï¼ˆæœ€åå…œåº•ï¼‰
        if "æ¨å¹¿éƒ¨é—¨" in parse_df.columns and parse_df["ç›˜å£"].isna().any():
            parse_df["ç›˜å£"] = parse_df["ç›˜å£"].fillna(parse_df["æ¨å¹¿éƒ¨é—¨"].map(DEPT_TO_PLATFORM))
            print(f"  ä»éƒ¨é—¨æ˜ å°„è¡¥å……: {parse_df['ç›˜å£'].notna().sum()} æ¡")

    # 4) ç»„å»ºä¸»è¡¨å¹¶å·¦è¿æ¥å„æŒ‡æ ‡
    main = parse_df.copy()  # å«ï¼šæ—¥æœŸã€æ€»ä»£åç§°(_æ¸…æ´—)ã€æ€»ä»£å·ã€äº§å“ã€æ¨å¹¿éƒ¨é—¨ã€æ¨å¹¿æ–¹å¼ã€ç›˜å£(å¯èƒ½ä¸ºç©º)
    print(f"  Main table initialized: {len(main)} rows, {len(main.columns)} columns")
    
    # V3.3: æ•°æ®éªŒè¯æ£€æŸ¥ç‚¹
    print("\n[æ•°æ®éªŒè¯ - Mainè¡¨åˆå§‹åŒ–å]")
    valid_ids = main["æ€»ä»£å·"].notna().sum()
    print(f"  æ€»ä»£å·éç©º: {valid_ids} / {len(main)}")
    if "ç›˜å£" in main.columns:
        valid_platforms = main["ç›˜å£"].notna().sum()
        print(f"  ç›˜å£éç©º: {valid_platforms} / {len(main)}")
        if valid_platforms > 0:
            unique_platforms = main["ç›˜å£"].dropna().unique().tolist()
            print(f"  ç›˜å£å”¯ä¸€å€¼({len(unique_platforms)}ä¸ª): {unique_platforms[:10]}")
    if "æ¨å¹¿éƒ¨é—¨" in main.columns:
        valid_depts = main["æ¨å¹¿éƒ¨é—¨"].notna().sum()
        print(f"  æ¨å¹¿éƒ¨é—¨éç©º: {valid_depts} / {len(main)}")
    
    # è‹¥ç¼ºIDï¼Œç”¨ç¨³å®šIDå…œåº•ï¼Œåç»­ä¸»é”®ä¸èšåˆæ‰å¯ç”¨
    if valid_ids < len(main):
        main["æ€»ä»£å·"] = main["æ€»ä»£å·"].where(
            main["æ€»ä»£å·"].notna(),
            main["æ€»ä»£åç§°_æ¸…æ´—"].map(stable_agent_id)
        )
        print(f"  After stable ID fill: {main['æ€»ä»£å·'].notna().sum()} / {len(main)} have IDs")
    
    # Determine merge strategy: if no valid IDs, use name_clean instead
    use_id_merge = main["æ€»ä»£å·"].notna().sum() > 0
    merge_keys = ["æ—¥æœŸ", "æ€»ä»£å·"] if use_id_merge else ["æ—¥æœŸ", "æ€»ä»£åç§°_æ¸…æ´—"]
    print(f"  Using merge keys: {'date+id' if use_id_merge else 'date+name_clean'}")
    
    # ä»£ç†ï¼šæ³¨å†Œ/æ´»è·ƒ/å……å€¼/é¦–å……/æç°
    if not agent.empty:
        print(f"\n[Agentæ•°æ®åˆå¹¶]")
        print(f"  Agentæ•°æ®è¡Œæ•°: {len(agent)}")
        agent_merge_cols = [c for c in merge_keys if c in agent.columns]
        if agent_merge_cols and all(k in agent.columns for k in merge_keys):
            # åªé€‰æ‹©å­˜åœ¨çš„åˆ—
            data_cols = [c for c in ["æ³¨å†Œäººæ•°","æ´»è·ƒäººæ•°","å……å€¼äººæ•°","å……å€¼é‡‘é¢","é¦–å……äººæ•°","å½“æ—¥é¦–å……é‡‘é¢","æç°é‡‘é¢","å……æå·®"] if c in agent.columns]
            print(f"  å¯ç”¨æ•°æ®åˆ—: {data_cols}")
            
            if data_cols:
                tmp = agent[merge_keys + data_cols].copy()
                # æ³¨æ„ï¼šagentæ•°æ®å·²ç»åœ¨å‰é¢æŒ‰date+idå»é‡äº†ï¼Œè¿™é‡Œä¸éœ€è¦å†å»é‡
                
                # V3.3: è¯¦ç»†è°ƒè¯•ä¿¡æ¯
                print(f"  [åˆå¹¶å‰] Agentæ•°æ®: {len(tmp)} è¡Œ")
                if 'å……å€¼é‡‘é¢' in tmp.columns:
                    non_zero = (tmp['å……å€¼é‡‘é¢'] > 0).sum()
                    print(f"  [åˆå¹¶å‰] å……å€¼é‡‘é¢éé›¶è¡Œ: {non_zero}, æ€»å’Œ: {tmp['å……å€¼é‡‘é¢'].sum():.2f}")
                if 'æ³¨å†Œäººæ•°' in tmp.columns:
                    non_zero = (tmp['æ³¨å†Œäººæ•°'] > 0).sum()
                    print(f"  [åˆå¹¶å‰] æ³¨å†Œäººæ•°éé›¶è¡Œ: {non_zero}, æ€»å’Œ: {tmp['æ³¨å†Œäººæ•°'].sum()}")
                
                # æ£€æŸ¥åˆå¹¶é”®åŒ¹é…
                main_keys_set = set(zip(main[merge_keys[0]], main[merge_keys[1]]))
                tmp_keys_set = set(zip(tmp[merge_keys[0]], tmp[merge_keys[1]]))
                matched_keys = main_keys_set & tmp_keys_set
                print(f"  [åˆå¹¶é”®åŒ¹é…] Main={len(main_keys_set)}, Agent={len(tmp_keys_set)}, åŒ¹é…={len(matched_keys)}")
                
                if len(matched_keys) == 0:
                    print(f"  âŒ è­¦å‘Šï¼šåˆå¹¶é”®å®Œå…¨ä¸åŒ¹é…ï¼")
                    print(f"  Mainæ ·æœ¬é”®: {list(main_keys_set)[:3]}")
                    print(f"  Agentæ ·æœ¬é”®: {list(tmp_keys_set)[:3]}")
                
                # æ‰§è¡Œmerge
                before_rows = len(main)
                main = main.merge(tmp, on=merge_keys, how="left")
                print(f"  [åˆå¹¶å] Mainæ•°æ®: {before_rows} -> {len(main)} è¡Œ")
                
                # åˆå¹¶åçš„æ•°æ®
                if 'å……å€¼é‡‘é¢' in main.columns:
                    non_zero = (main['å……å€¼é‡‘é¢'] > 0).sum()
                    print(f"  [åˆå¹¶å] å……å€¼é‡‘é¢éé›¶è¡Œ: {non_zero}, æ€»å’Œ: {main['å……å€¼é‡‘é¢'].sum():.2f}")
                if 'æ³¨å†Œäººæ•°' in main.columns:
                    non_zero = (main['æ³¨å†Œäººæ•°'] > 0).sum()
                    print(f"  [åˆå¹¶å] æ³¨å†Œäººæ•°éé›¶è¡Œ: {non_zero}, æ€»å’Œ: {main['æ³¨å†Œäººæ•°'].sum()}")
            else:
                print(f"  âš ï¸ è·³è¿‡ï¼šæ²¡æœ‰å¯ç”¨çš„æ•°æ®åˆ—")
        else:
            print(f"  âš ï¸ è·³è¿‡ï¼šç¼ºå°‘åˆå¹¶é”® {merge_keys}")

    # æ—¥å¸¸ï¼šé¦–å……ã€æ´»è·ƒå……å€¼
    if not daily.empty:
        daily_keys = [k for k in merge_keys if k in daily.columns]
        if daily_keys and len(daily_keys) == len(merge_keys):
            cols = daily_keys + [c for c in ["å½“æ—¥é¦–å……é‡‘é¢","é¦–å……äººæ•°","æ´»è·ƒå……å€¼äººæ•°"] if c in daily.columns]
            tmp = daily[cols].copy()
            # æ³¨æ„ï¼šdailyæ•°æ®å·²ç»åœ¨å‰é¢æŒ‰date+idå»é‡äº†ï¼Œè¿™é‡Œä¸éœ€è¦å†å»é‡
            
            main = main.merge(tmp, on=daily_keys, how="left", suffixes=("",""))
    
    # V3.0: ä¸€çº§é¦–å……äººæ•°ï¼ˆV3.5.9: æ·»åŠ è¯¦ç»†è¯Šæ–­ï¼‰
    if not primary_firstpay.empty:
        print(f"\n[ä¸€çº§é¦–å……åˆå¹¶]")
        print(f"  primary_firstpayæ•°æ®è¡Œæ•°: {len(primary_firstpay)}")
        primary_keys = [k for k in merge_keys if k in primary_firstpay.columns]
        if primary_keys and len(primary_keys) == len(merge_keys):
            cols = primary_keys + ["ä¸€çº§é¦–å……äººæ•°"]
            tmp = primary_firstpay[cols].copy()
            
            # è¯Šæ–­ï¼šæ£€æŸ¥åˆå¹¶é”®åŒ¹é…
            main_keys_set = set(zip(*[main[k] for k in primary_keys]))
            tmp_keys_set = set(zip(*[tmp[k] for k in primary_keys]))
            matched_keys = main_keys_set & tmp_keys_set
            print(f"  åˆå¹¶é”®: {primary_keys}")
            print(f"  Mainé”®æ•°é‡: {len(main_keys_set)}, primary_firstpayé”®æ•°é‡: {len(tmp_keys_set)}, åŒ¹é…æ•°: {len(matched_keys)}")
            
            before_rows = len(main)
            main = main.merge(tmp, on=primary_keys, how="left")
            print(f"  åˆå¹¶å: {before_rows} -> {len(main)} rows")
            
            # æ£€æŸ¥ä¸€çº§é¦–å……äººæ•°åˆ—
            if "ä¸€çº§é¦–å……äººæ•°" in main.columns:
                non_zero = (main["ä¸€çº§é¦–å……äººæ•°"] > 0).sum()
                total = main["ä¸€çº§é¦–å……äººæ•°"].sum()
                print(f"  ä¸€çº§é¦–å……äººæ•°: {non_zero}/{len(main)} éé›¶, æ€»è®¡: {total}")
            else:
                print(f"  âš ï¸ è­¦å‘Šï¼šä¸€çº§é¦–å……äººæ•°åˆ—æœªæˆåŠŸåˆå¹¶")
        else:
            print(f"  âš ï¸ è·³è¿‡ï¼šåˆå¹¶é”®ä¸åŒ¹é… {primary_keys}")

    # æˆæœ¬/å¹¿å‘Šï¼ˆæ—¢æ”¯æŒæ¸ é“çº§ï¼Œä¹Ÿæ”¯æŒå¹³å°çº§ï¼‰
    # æ¸ é“çº§
    if not cost.empty and "æ€»ä»£åç§°_æ¸…æ´—" in cost.columns:
        tmp = cost[["æ—¥æœŸ","æ€»ä»£åç§°_æ¸…æ´—","æ¶ˆè€—","å±•ç¤º","ç‚¹å‡»","æç°é‡‘é¢"]].copy()
        
        # ğŸ”§ ä¿®å¤ï¼šåˆå¹¶å‰å»é‡
        data_cols = ["æ¶ˆè€—","å±•ç¤º","ç‚¹å‡»","æç°é‡‘é¢"]
        data_cols = [c for c in data_cols if c in tmp.columns]
        if data_cols:
            tmp = tmp.groupby(["æ—¥æœŸ","æ€»ä»£åç§°_æ¸…æ´—"], as_index=False)[data_cols].sum()
            print(f"  Cost (channel) data after dedup: {len(tmp)} rows")
        
        main = main.merge(tmp, on=["æ—¥æœŸ","æ€»ä»£åç§°_æ¸…æ´—"], how="left")
    # å¹³å°çº§ï¼ˆå¹¿æ’­ï¼‰
    if not cost.empty and "ç›˜å£" in cost.columns:
        tmp = cost[["æ—¥æœŸ","ç›˜å£","æ¶ˆè€—","å±•ç¤º","ç‚¹å‡»","æç°é‡‘é¢"]].copy()
        
        # ğŸ”§ ä¿®å¤ï¼šåˆå¹¶å‰å»é‡
        data_cols = ["æ¶ˆè€—","å±•ç¤º","ç‚¹å‡»","æç°é‡‘é¢"]
        data_cols = [c for c in data_cols if c in tmp.columns]
        if data_cols:
            tmp = tmp.groupby(["æ—¥æœŸ","ç›˜å£"], as_index=False)[data_cols].sum()
            print(f"  Cost (platform) data after dedup: {len(tmp)} rows")
        
        # é˜²æ­¢è¦†ç›–æ¸ é“çº§çš„éç©ºæ•°å€¼ï¼šåªåœ¨ç©ºå€¼å¤„ç”¨å¹³å°çº§è¡¥
        main = main.merge(tmp, on=["æ—¥æœŸ","ç›˜å£"], how="left", suffixes=("","_plat"))
        for col in ["æ¶ˆè€—","å±•ç¤º","ç‚¹å‡»","æç°é‡‘é¢"]:
            if col+"_plat" in main.columns:
                main[col] = main[col].fillna(main[col+"_plat"])
                main.drop(columns=[col+"_plat"], inplace=True)

    # å¹³å°LTVï¼ˆå¹¿æ’­ï¼‰
    if not platform.empty:
        tmp = platform.copy()
        # æ˜ å°„åˆ°æ ‡å‡†åˆ—å
        rename_map = {"ltv_D1":"é¦–å……å½“æ—¥ltvæ›¿ä»£_D1",
                      "ltv_D3":"å¹³å°LTV_D3","ltv_D7":"å¹³å°LTV_D7","ltv_D14":"å¹³å°LTV_D14","ltv_D30":"å¹³å°LTV_D30"}
        tmp.rename(columns=rename_map, inplace=True)
        
        # ğŸ”§ ä¿®å¤ï¼šåˆå¹¶å‰å»é‡
        merge_cols = ["æ—¥æœŸ","ç›˜å£"]
        data_cols = [c for c in list(rename_map.values()) if c in tmp.columns]
        if data_cols:
            tmp = tmp[merge_cols + data_cols].groupby(merge_cols, as_index=False)[data_cols].mean()
            print(f"  Platform LTV data after dedup: {len(tmp)} rows")
        
        main = main.merge(tmp, on=["æ—¥æœŸ","ç›˜å£"], how="left")

    # V3.5.14: é¦–å……LTVå€’æ¨ï¼ˆä½¿ç”¨fpltv_historicalå†å²æ•°æ®ï¼‰
    if not fpltv_historical.empty:
        if "æ—¥æœŸ" in fpltv_historical.columns and 'selected_date' in locals():
            print(f"\n[FPLTVå€’æ¨] æŒ‰åç§»æ—¥ç²¾ç¡®å–å€¼ï¼ˆä¸æ”¹æ—¥æœŸï¼Œä»…å–å€¼ï¼‰...")
            fpltv_historical["æ—¥æœŸ_dt"] = pd.to_datetime(fpltv_historical["æ—¥æœŸ"])
            selected_dt = pd.to_datetime(selected_date)

            # åˆ—è¯†åˆ«è¯Šæ–­
            ltv_cols_all = [c for c in ["FPLTV_D1","FPLTV_D2","FPLTV_D3","FPLTV_D7","FPLTV_D14","FPLTV_D15","FPLTV_D30"] if c in fpltv_historical.columns]
            print(f"  [FPLTVåˆ—è¯†åˆ«] å·²è¯†åˆ«åˆ—: {ltv_cols_all}")

            # éœ€è¦çš„åç§»å¤©æ•°ä¸åˆ—æ˜ å°„
            need_map = {
                1: "FPLTV_D1",
                2: "FPLTV_D2",
                3: "FPLTV_D3",
                7: "FPLTV_D7",
                14: "FPLTV_D14",
                15: "FPLTV_D15",
                30: "FPLTV_D30",
            }

            # åˆå¹¶é”®ï¼ˆä¸ä½¿ç”¨æ—¥æœŸï¼‰
            merge_keys = []
            if "æ€»ä»£å·" in fpltv_historical.columns and "æ€»ä»£å·" in main.columns and "ç›˜å£" in fpltv_historical.columns and "ç›˜å£" in main.columns:
                merge_keys = ["ç›˜å£","æ€»ä»£å·"]
            elif "ç›˜å£" in fpltv_historical.columns and "ç›˜å£" in main.columns:
                merge_keys = ["ç›˜å£"]
            else:
                print("  [FPLTV] ç¼ºå°‘åˆå¹¶é”®ï¼ˆç›˜å£/æ€»ä»£å·ï¼‰ï¼Œè·³è¿‡")
                merge_keys = []

            # é’ˆå¯¹æ¯ä¸ªåç§»æ—¥ï¼Œç²¾ç¡®å–ç›®æ ‡æ—¥æœŸ = selected_date - n çš„ä¸€è¡Œï¼Œå†è´´æ•°å€¼
            if merge_keys:
                for offset, col in need_map.items():
                    if col not in fpltv_historical.columns:
                        print(f"  [FPLTV] ç¼ºå°‘åˆ— {col}ï¼Œè·³è¿‡è¯¥åç§»{offset}æ—¥")
                        continue
                    cutoff_dt = selected_dt - pd.Timedelta(days=offset)
                    sub = fpltv_historical[fpltv_historical["æ—¥æœŸ_dt"] <= cutoff_dt]
                    if sub.empty:
                        print(f"  [FPLTV] åç§»{offset}æ—¥åœ¨ â‰¤ {cutoff_dt.date()} æ— å¯ç”¨è¡Œ")
                        continue
                    # å–æ¯ä¸ªé”®åœ¨æˆªæ­¢æ—¥å‰çš„æœ€æ–°ä¸€è¡Œ
                    sub = sub.sort_values("æ—¥æœŸ_dt").groupby(merge_keys, as_index=False).last()
                    # åªä¿ç•™éœ€è¦çš„åˆ—
                    keep_cols = [c for c in (merge_keys + [col]) if c in sub.columns]
                    sub = sub[keep_cols].copy()
                    print(f"  [FPLTV] åç§»{offset}æ—¥å¯ç”¨: {len(sub)} è¡Œï¼ˆæˆªæ­¢ {cutoff_dt.date()}ï¼‰")
                    main = main.merge(sub, on=merge_keys, how="left")

                # è¯Šæ–­ï¼šè´´å®Œåç»Ÿè®¡éé›¶
                for col in ["FPLTV_D7","FPLTV_D14","FPLTV_D15","FPLTV_D30"]:
                    if col in main.columns:
                        non_zero = (pd.to_numeric(main[col], errors='coerce').fillna(0) != 0).sum()
                        print(f"  [FPLTV] åˆå¹¶å {col}: éé›¶ {non_zero}/{len(main)}")
        else:
            print(f"  [è­¦å‘Š] FPLTVç¼ºå°‘æ—¥æœŸåˆ—æˆ–æœªè®¾ç½®selected_dateï¼Œè·³è¿‡å€’æ¨")

    # V3.5.9: ç•™å­˜ï¼ˆç”¨äºå¤ç™»ç‡_åç§»ï¼šä½¿ç”¨_historicalç‰ˆæœ¬è¿›è¡Œå€’æ¨å–å€¼ï¼‰
    # ä¼˜å…ˆç”¨é¦–å……ç•™å­˜ï¼›ç¼ºåˆ™ç”¨ä¸‹æ³¨ç•™å­˜ï¼›å†ç¼ºç”¨é¦–ç™»ç•™å­˜ï¼›æœ€åç”¨æ³¨å†Œç•™å­˜
    # v2.1: æ–°å¢ret_playé€‰é¡¹
    ret_pick = {}
    if not ret_fpay_historical.empty:
        ret_pick = ret_fpay_historical.copy()
    elif not ret_play_historical.empty:  # v2.1: æ–°å¢ä¼˜å…ˆçº§
        ret_pick = ret_play_historical.copy()
    elif not ret_login_historical.empty:
        ret_pick = ret_login_historical.copy()
    elif not ret_register_historical.empty:
        ret_pick = ret_register_historical.copy()
    if isinstance(ret_pick, pd.DataFrame) and not ret_pick.empty:
        # V3.5.11: ä¸å†ä½¿ç”¨æ—¥æœŸä½œä¸ºåˆå¹¶é”®ï¼Œåªç”¨[ç›˜å£, æ€»ä»£å·]æˆ–[ç›˜å£]
        merge_keys = []
        if "æ€»ä»£å·" in ret_pick.columns and "æ€»ä»£å·" in main.columns:
            # ä¼˜å…ˆä½¿ç”¨æ€»ä»£å·ç²¾ç¡®åŒ¹é…
            if "ç›˜å£" in ret_pick.columns and "ç›˜å£" in main.columns:
                merge_keys = ["ç›˜å£", "æ€»ä»£å·"]
            else:
                merge_keys = ["æ€»ä»£å·"]
            print(f"  [ç•™å­˜] ä½¿ç”¨[ç›˜å£+æ€»ä»£å·]ç»´åº¦åˆå¹¶")
        elif "ç›˜å£" in ret_pick.columns and "ç›˜å£" in main.columns:
            # é™çº§ï¼šæŒ‰ç›˜å£å¹¿æ’­
            merge_keys = ["ç›˜å£"]
            print(f"  [ç•™å­˜] é™çº§ä¸º[ç›˜å£]ç»´åº¦åˆå¹¶")
        
        keep = merge_keys + [c for c in ret_pick.columns if any(k in c for k in ["D1","D3","D7","D15","D30"])]
        keep = [c for c in keep if c in ret_pick.columns]  # Filter to existing columns
        tmp = ret_pick[keep].copy()
        
        # ğŸ”§ ä¿®å¤ï¼šåˆå¹¶å‰å»é‡ï¼Œé¿å…é‡å¤æ–‡ä»¶å¯¼è‡´ç¬›å¡å°”ç§¯
        # æŒ‰åˆå¹¶é”®å»é‡ï¼Œå¯¹æ•°å€¼åˆ—å–å¹³å‡å€¼
        if len(merge_keys) >= 1:
            numeric_cols = [c for c in tmp.columns if c not in merge_keys]
            if numeric_cols:
                # èšåˆï¼šå¯¹é‡å¤çš„åˆå¹¶é”®ï¼Œæ•°å€¼åˆ—å–å¹³å‡
                tmp = tmp.groupby(merge_keys, as_index=False)[numeric_cols].mean()
            else:
                # å¦‚æœæ²¡æœ‰æ•°å€¼åˆ—ï¼Œç›´æ¥å»é‡
                tmp = tmp.drop_duplicates(subset=merge_keys, keep='first')
            
            print(f"  [ç•™å­˜] å»é‡åæ•°æ®: {len(tmp)} è¡Œ")
            print(f"  [ç•™å­˜] åˆå¹¶é”®: {merge_keys}, æ•°æ®è¡Œæ•°: {len(tmp)}")
            main = main.merge(tmp, on=merge_keys, how="left", suffixes=("","_ret"))
        else:
            print(f"  [è­¦å‘Š] ç•™å­˜æ•°æ®ç¼ºå°‘æœ‰æ•ˆåˆå¹¶é”®ï¼Œè·³è¿‡åˆå¹¶")

    # 5) æŒ‡æ ‡è®¡ç®—ä¸ç¼ºçœå¤„ç†
    def num(col_name, default_val=0): 
        if col_name in main.columns:
            return pd.to_numeric(main[col_name], errors="coerce").fillna(default_val)
        else:
            return pd.Series([default_val] * len(main), index=main.index)

    # è¡ç”Ÿ
    main["æ³¨å†Œäººæ•°"]     = num("æ³¨å†Œäººæ•°", 0).astype("Int64")
    main["æ´»è·ƒäººæ•°"]     = num("æ´»è·ƒäººæ•°", 0).astype("Int64")
    main["å……å€¼äººæ•°"]     = num("å……å€¼äººæ•°", 0).astype("Int64")
    main["å……å€¼é‡‘é¢"]     = num("å……å€¼é‡‘é¢", 0.0)
    main["å½“æ—¥é¦–å……é‡‘é¢"] = num("å½“æ—¥é¦–å……é‡‘é¢", 0.0)
    main["é¦–å……äººæ•°"]     = num("é¦–å……äººæ•°", 0).astype("Int64")
    main["å±•ç¤º"]         = num("å±•ç¤º", 0).astype("Int64")
    main["ç‚¹å‡»"]         = num("ç‚¹å‡»", 0).astype("Int64")
    main["æ¶ˆè€—"]         = num("æ¶ˆè€—", 0.0)
    main["æç°é‡‘é¢"]     = num("æç°é‡‘é¢", 0.0)

    # â€”â€” å……æå·®ï¼šå¦‚æœæç°é‡‘é¢æœ‰æ¥æºï¼šå……å€¼é‡‘é¢ - æç°é‡‘é¢ï¼›å¦åˆ™æŒ‰ 0ï¼ˆæˆ–ä½¿ç”¨ä½ ä»¬çš„å…¶ä»–å…¬å¼ï¼‰
    main["å……æå·®"] = (main["å……å€¼é‡‘é¢"] - main["æç°é‡‘é¢"]).fillna(0.0)

    # åƒå±•æˆæœ¬crm
    main["åƒå±•æˆæœ¬crm"] = main.apply(lambda r: (r["æ¶ˆè€—"] / (r["å±•ç¤º"]/1000.0)) if r["å±•ç¤º"]>0 else 0.0, axis=1)
    # ç‚¹å‡»ç‡
    main["ç‚¹å‡»ç‡"] = main.apply(lambda r: (r["ç‚¹å‡»"] / r["å±•ç¤º"]) if r["å±•ç¤º"]>0 else 0.0, axis=1)
    # æ³¨å†Œæˆæœ¬ / é¦–å……æˆæœ¬
    main["æ³¨å†Œæˆæœ¬"]   = main.apply(lambda r: (r["æ¶ˆè€—"] / r["æ³¨å†Œäººæ•°"]) if r["æ³¨å†Œäººæ•°"] and r["æ³¨å†Œäººæ•°"]>0 else 0.0, axis=1)
    main["é¦–å……æˆæœ¬"]   = main.apply(lambda r: (r["æ¶ˆè€—"] / r["é¦–å……äººæ•°"]) if r["é¦–å……äººæ•°"] and r["é¦–å……äººæ•°"]>0 else 0.0, axis=1)
    # V3.5.9: ä¸€çº§é¦–å……äººæ•°/æˆæœ¬ï¼ˆä»primary_firstpayåˆå¹¶ï¼Œå·²åœ¨å‰é¢å¤„ç†ï¼‰
    # å¦‚æœåˆå¹¶å¤±è´¥ï¼Œå…œåº•ä¸º0
    if "ä¸€çº§é¦–å……äººæ•°" not in main.columns:
        main["ä¸€çº§é¦–å……äººæ•°"] = 0
    main["ä¸€çº§é¦–å……äººæ•°"] = num("ä¸€çº§é¦–å……äººæ•°", 0).astype("Int64")
    main["ä¸€çº§é¦–å……æˆæœ¬"] = main.apply(lambda r: (r["æ¶ˆè€—"] / r["ä¸€çº§é¦–å……äººæ•°"]) if r["ä¸€çº§é¦–å……äººæ•°"] and r["ä¸€çº§é¦–å……äººæ•°"]>0 else 0.0, axis=1)

    # é¦–å……è½¬åŒ–ç‡ / é¦–å……arppu / é¦–å……roas
    main["é¦–å……è½¬åŒ–ç‡"] = main.apply(lambda r: (r["é¦–å……äººæ•°"]/r["æ³¨å†Œäººæ•°"]) if r["æ³¨å†Œäººæ•°"] and r["æ³¨å†Œäººæ•°"]>0 else 0.0, axis=1)
    main["é¦–å……arppu"] = main.apply(lambda r: (r["å½“æ—¥é¦–å……é‡‘é¢"]/r["é¦–å……äººæ•°"]) if r["é¦–å……äººæ•°"] and r["é¦–å……äººæ•°"]>0 else 0.0, axis=1)
    main["é¦–å……roas"] = main.apply(lambda r: (r["å½“æ—¥é¦–å……é‡‘é¢"]/r["æ¶ˆè€—"]) if r["æ¶ˆè€—"]>0 else 0.0, axis=1)

    # é¦–å……å½“æ—¥ltvï¼ˆä¼˜å…ˆç”¨é¦–å……LTV_D1ï¼›æ²¡æœ‰å°±ç”¨å¹³å°LTV_D1 æ›¿ä»£ï¼‰
    main["é¦–å……å½“æ—¥ltv"] = num("FPLTV_D1", 0.0)

    # é¦–å……å½“æ—¥roiï¼ˆæŒ‰ä½ çš„å£å¾„å¯æ”¹ï¼›è¿™é‡Œç¤ºä¾‹ç”¨ï¼šé¦–å……å½“æ—¥å……æå·® / æ¶ˆè€—ï¼‰
    # å…ˆç®— é¦–å……å½“æ—¥å……æå·® = å½“æ—¥é¦–å……é‡‘é¢ - å½“æ—¥é¦–æé‡‘é¢
    # V3.5.10: æ”¯æŒ scale æ¨¡å¼ä¼°ç®—é¦–å……ç”¨æˆ·æç°
    if WITHDRAW_APPROX_MODE == "scale":
        # æŒ‰æ¯”ä¾‹ä¼°ç®—ï¼šé¦–å……ç”¨æˆ·æç° â‰ˆ æ€»æç° Ã— (é¦–å……äººæ•°/å……å€¼äººæ•°)
        withdraw_ratio = main.apply(lambda r: r["é¦–å……äººæ•°"]/r["å……å€¼äººæ•°"] if r["å……å€¼äººæ•°"]>0 else 0, axis=1)
        estimated_fpay_withdraw = main["æç°é‡‘é¢"] * withdraw_ratio
        main["é¦–å……å½“æ—¥å……æå·®"] = (main["å½“æ—¥é¦–å……é‡‘é¢"] - estimated_fpay_withdraw).fillna(0.0)
        print(f"  [é¦–å……å……æå·®] ä½¿ç”¨scaleæ¨¡å¼ä¼°ç®—ï¼ˆé¦–å……äººæ•°/å……å€¼äººæ•°æ¯”ä¾‹ï¼‰")
    else:
        # zeroæ¨¡å¼ï¼šä¸ä¼°ç®—æç°ï¼Œé¦–å……å……æå·®=é¦–å……é‡‘é¢
        main["é¦–å……å½“æ—¥å……æå·®"] = main["å½“æ—¥é¦–å……é‡‘é¢"]
        print(f"  [é¦–å……å……æå·®] ä½¿ç”¨zeroæ¨¡å¼ï¼ˆä¸ä¼°ç®—æç°ï¼‰")
    
    main["é¦–å……å½“æ—¥roi"] = main.apply(lambda r: (r["é¦–å……å½“æ—¥å……æå·®"]/r["æ¶ˆè€—"]) if r["æ¶ˆè€—"]>0 else 0.0, axis=1)

    # é¦–å……å……æå·®æ¯” = é¦–å……å½“æ—¥å……æå·® / å½“æ—¥é¦–å……é‡‘é¢
    main["é¦–å……å……æå·®æ¯”"] = main.apply(lambda r: (r["é¦–å……å½“æ—¥å……æå·®"]/r["å½“æ—¥é¦–å……é‡‘é¢"]) if r["å½“æ—¥é¦–å……é‡‘é¢"]>0 else 0.0, axis=1)

    # åç§» LTVï¼ˆæ¥è‡ª FPLTVï¼‰
    main["é¦–å……ä¸¤æ—¥ltv_åç§»"]   = num("FPLTV_D2", 0.0)
    main["é¦–å……ä¸‰æ—¥ltv_åç§»"]   = num("FPLTV_D3", 0.0)
    main["é¦–å……ä¸ƒæ—¥ltv_åç§»"]   = num("FPLTV_D7", 0.0)
    # 15æ—¥ï¼šä¼˜å…ˆç”¨D15ï¼Œç¼ºåˆ™ç”¨D14
    if "FPLTV_D15" in main.columns:
        main["é¦–å……åäº”æ—¥ltv_åç§»"] = num("FPLTV_D15", 0.0)
    else:
        main["é¦–å……åäº”æ—¥ltv_åç§»"] = 0.0
    if ("FPLTV_D14" in main.columns):
        main["é¦–å……åäº”æ—¥ltv_åç§»"] = main["é¦–å……åäº”æ—¥ltv_åç§»"].where(
            pd.to_numeric(main["é¦–å……åäº”æ—¥ltv_åç§»"], errors='coerce').fillna(0) != 0,
            num("FPLTV_D14", 0.0)
        )
    main["é¦–å……ä¸‰åæ—¥ltv_åç§»"] = num("FPLTV_D30", 0.0)

    # åç§» å¤ç™»/å¤æŠ•/å¤å……ç‡ï¼šä¸¥æ ¼åˆ†æ¥æºï¼›æ”¯æŒä¸¤ç§å£å¾„ï¼ˆretention/formulaï¼‰
    def _get_ret(prefix: str, day: int):
        col = f"{prefix}_D{day}"
        return num(col, 0.0) if col in main.columns else pd.Series(0.0, index=main.index)

    reg = pd.to_numeric(main["æ³¨å†Œäººæ•°"], errors="coerce")
    fpu = pd.to_numeric(main["é¦–å……äººæ•°"], errors="coerce")
    ratio = (fpu / reg.replace(0, pd.NA)).fillna(0.0)

    def _apply_offset(series: pd.Series) -> pd.Series:
        if OFFSET_MODE == "formula":
            return (series * ratio).fillna(0.0)
        return series.fillna(0.0)

    # å¤ç™»ç‡_åç§» â† ret_login
    main["é¦–å……æ¬¡æ—¥å¤ç™»ç‡_åç§»"]   = _apply_offset(_get_ret("ret_login", 1))
    main["é¦–å……ä¸‰æ—¥å¤ç™»ç‡_åç§»"]   = _apply_offset(_get_ret("ret_login", 3))
    main["é¦–å……ä¸ƒæ—¥å¤ç™»ç‡_åç§»"]   = _apply_offset(_get_ret("ret_login", 7))
    main["é¦–å……åäº”æ—¥å¤ç™»ç‡_åç§»"] = _apply_offset(_get_ret("ret_login", 15))
    main["é¦–å……ä¸‰åæ—¥å¤ç™»ç‡_åç§»"] = _apply_offset(_get_ret("ret_login", 30))

    # å¤æŠ•ç‡_åç§» â† ret_play
    main["é¦–å……æ¬¡æ—¥å¤æŠ•ç‡_åç§»"]   = _apply_offset(_get_ret("ret_play", 1))
    main["é¦–å……ä¸‰æ—¥å¤æŠ•ç‡_åç§»"]   = _apply_offset(_get_ret("ret_play", 3))
    main["é¦–å……ä¸ƒæ—¥å¤æŠ•ç‡_åç§»"]   = _apply_offset(_get_ret("ret_play", 7))
    main["é¦–å……åäº”æ—¥å¤æŠ•ç‡_åç§»"] = _apply_offset(_get_ret("ret_play", 15))
    main["é¦–å……ä¸‰åæ—¥å¤æŠ•ç‡_åç§»"] = _apply_offset(_get_ret("ret_play", 30))

    # å¤å……ç‡_åç§» â† ret_fpay
    main["é¦–å……æ¬¡æ—¥å¤å……ç‡_åç§»"]   = _apply_offset(_get_ret("ret_fpay", 1))
    main["é¦–å……ä¸‰æ—¥å¤å……ç‡_åç§»"]   = _apply_offset(_get_ret("ret_fpay", 3))
    main["é¦–å……ä¸ƒæ—¥å¤å……ç‡_åç§»"]   = _apply_offset(_get_ret("ret_fpay", 7))
    main["é¦–å……åäº”æ—¥å¤å……ç‡_åç§»"] = _apply_offset(_get_ret("ret_fpay", 15))
    main["é¦–å……ä¸‰åæ—¥å¤å……ç‡_åç§»"] = _apply_offset(_get_ret("ret_fpay", 30))

    # ç´¯è®¡roas / è‡ªç„¶æœˆæ¶ˆè€—ï¼ˆæŒ‰çª—å£ç´¯è®¡ï¼‰
    # V3.5.10: å®ç°çœŸå®ç´¯è®¡ROASè®¡ç®—
    print(f"  [ç´¯è®¡ROAS] è®¡ç®—æŒ‰æ€»ä»£å·+ç›˜å£çš„ç´¯è®¡å……å€¼å’Œæ¶ˆè€—...")
    main = main.sort_values(["æ€»ä»£å·", "ç›˜å£", "æ—¥æœŸ"])
    main["ç´¯è®¡å……å€¼é‡‘é¢"] = main.groupby(["æ€»ä»£å·", "ç›˜å£"])["å……å€¼é‡‘é¢"].cumsum()
    main["ç´¯è®¡æ¶ˆè€—"] = main.groupby(["æ€»ä»£å·", "ç›˜å£"])["æ¶ˆè€—"].cumsum()
    main["ç´¯è®¡roas"] = main.apply(
        lambda r: r["ç´¯è®¡å……å€¼é‡‘é¢"]/r["ç´¯è®¡æ¶ˆè€—"] if r["ç´¯è®¡æ¶ˆè€—"]>0 else 0.0, axis=1)
    
    # è‡ªç„¶æœˆæ¶ˆè€—
    main["è‡ªç„¶æœˆ"] = main["æ—¥æœŸ"].str.slice(0, 7)
    main["è‡ªç„¶æœˆæ¶ˆè€—"] = main.groupby(["æ€»ä»£å·", "ç›˜å£", "è‡ªç„¶æœˆ"])["æ¶ˆè€—"].transform("sum")

    # V3.5.9: éä¸€çº§é¦–å……å æ¯”ï¼ˆè®¡ç®—å…¬å¼ï¼‰
    main["éä¸€çº§é¦–å……äººæ•°"] = (main["é¦–å……äººæ•°"] - main["ä¸€çº§é¦–å……äººæ•°"]).clip(lower=0).astype("Int64")
    main["éä¸€çº§é¦–å……äººæ•°/é¦–å……äººæ•°"] = main.apply(
        lambda r: (r["éä¸€çº§é¦–å……äººæ•°"]/r["é¦–å……äººæ•°"]) if r["é¦–å……äººæ•°"] and r["é¦–å……äººæ•°"]>0 else 0.0, axis=1)
    main["éä¸€çº§é¦–å……äººæ•°/å……å€¼äººæ•°"] = main.apply(
        lambda r: (r["éä¸€çº§é¦–å……äººæ•°"]/r["å……å€¼äººæ•°"]) if r["å……å€¼äººæ•°"] and r["å……å€¼äººæ•°"]>0 else 0.0, axis=1)

    # 6) å¡«å……å…¶ä»–ç»´åº¦åˆ—
    main["äº§å“"] = main["äº§å“"].fillna("")
    main["æ¨å¹¿æ–¹å¼"] = main["æ¨å¹¿æ–¹å¼"].fillna("")
    main["æ€»ä»£åç§°"] = main["æ€»ä»£åç§°"].fillna(main["æ€»ä»£åç§°_æ¸…æ´—"])
    # æ—¥æœŸå¿…é¡»å­˜åœ¨ï¼Œæ€»ä»£å·æˆ–æ€»ä»£åç§°è‡³å°‘æœ‰ä¸€ä¸ª
    main = main.dropna(subset=["æ—¥æœŸ"])
    # If no valid IDs, at least need name_clean
    if valid_ids == 0:
        main = main.dropna(subset=["æ€»ä»£åç§°_æ¸…æ´—"])

    # 7) è¾“å‡º 53 åˆ—ï¼ˆç¼ºå¤±åˆ—è¡¥ 0/ç©ºï¼‰ï¼Œé¡ºåºé”å®š
    print(f"  Before adding missing columns: {len(main)} rows")
    for col in FINAL_COLUMNS:
        if col not in main.columns:
            # é»˜è®¤ç¼ºå¤±ï¼šæ•°å€¼ 0ï¼Œæ–‡æœ¬ç©º
            if col in ["æ—¥æœŸ","äº§å“","ç›˜å£","æ€»ä»£åç§°","æ¨å¹¿éƒ¨é—¨","æ¨å¹¿æ–¹å¼"]:
                main[col] = ""
            else:
                main[col] = 0.0
    print(f"  After adding missing columns: {len(main)} rows")

    # åˆ—ç±»å‹å¾®è°ƒï¼šæ•´æ•°åˆ—
    for icol in ["æ³¨å†Œäººæ•°","é¦–å……äººæ•°","ä¸€çº§é¦–å……äººæ•°","å……å€¼äººæ•°","å±•ç¤º","ç‚¹å‡»"]:
        if icol in main.columns:
            main[icol] = pd.to_numeric(main[icol], errors="coerce").fillna(0).astype("Int64")

    # èšåˆä¸å»é‡ï¼šç»Ÿä¸€æŒ‰ [æ—¥æœŸ, ç›˜å£, æ€»ä»£å·] è¿›è¡Œåˆ†ç»„èšåˆ
    print(f"  Before aggregation: {len(main)} rows")
    
    # è°ƒè¯•ï¼šèšåˆå‰çš„æ•°æ®ç»Ÿè®¡
    if 'å……å€¼é‡‘é¢' in main.columns:
        print(f"  Before agg - å……å€¼é‡‘é¢æ€»å’Œ: {main['å……å€¼é‡‘é¢'].sum():.2f}")
    
    if len(main) > 0:
        group_cols = [c for c in ["æ—¥æœŸ", "ç›˜å£", "æ€»ä»£å·"] if c in main.columns]
        if group_cols:
            # V3.5.8: ç®€åŒ–ä¸”å¥å£®çš„æ ‡é‡æå–é€»è¾‘
            def flatten_to_scalar(value):
                """é€’å½’å±•å¼€DataFrame/Series/å®¹å™¨ç±»å‹è‡³æ ‡é‡ï¼Œæœ€å¤š10å±‚é˜²æ­¢æ— é™å¾ªç¯"""
                import numpy as np
                for _ in range(10):  # æœ€å¤š10æ¬¡é€’å½’
                    # DataFrame â†’ å–[0,0]
                    if isinstance(value, pd.DataFrame):
                        if value.shape[0] > 0 and value.shape[1] > 0:
                            value = value.iloc[0, 0]
                            continue
                        return None
                    
                    # Series â†’ å–ç¬¬ä¸€ä¸ªéç©ºï¼ˆæˆ–ç¬¬ä¸€ä¸ªï¼‰
                    if isinstance(value, pd.Series):
                        if len(value) == 0:
                            return None
                        non_null = value.dropna()
                        value = non_null.iloc[0] if len(non_null) > 0 else value.iloc[0]
                        continue
                    
                    # å®¹å™¨ç±»å‹ï¼ˆlist/tuple/ndarrayï¼‰ â†’ å–ç¬¬ä¸€ä¸ªéç©ºå…ƒç´ 
                    if isinstance(value, (list, tuple, np.ndarray)):
                        if len(value) == 0:
                            return None
                        seq = list(value)
                        for item in seq:
                            if item is not None and not (isinstance(item, float) and pd.isna(item)):
                                value = item
                                break
                        else:
                            value = seq[0]
                        continue
                    
                    # dict â†’ å–ç¬¬ä¸€ä¸ªvalue
                    if isinstance(value, dict):
                        try:
                            value = next(iter(value.values()))
                            continue
                        except StopIteration:
                            return None
                    
                    # å·²æ˜¯æ ‡é‡ï¼Œè·³å‡ºå¾ªç¯
                    break
                
                # æœ€ç»ˆæ£€æŸ¥ä¸è¿”å›
                if value is None or (isinstance(value, float) and pd.isna(value)):
                    return None
                
                # ç¡®ä¿æ˜¯Pythonæ ‡é‡ç±»å‹
                import numpy as np
                if isinstance(value, (str, bytes, int, float, bool)):
                    return value
                if hasattr(np, 'isscalar') and np.isscalar(value):
                    return value
                
                # æœ€åå…œåº•ï¼šå¼ºåˆ¶è½¬å­—ç¬¦ä¸²
                return str(value)

            def safe_first(series):
                """å®‰å…¨åœ°å–ç¬¬ä¸€ä¸ªéç©ºå€¼ï¼ˆç¡®ä¿è¿”å›æ ‡é‡ï¼ŒV3.5.8å¼ºåŒ–ç‰ˆ + æœ€ç»ˆå…œåº•ï¼‰"""
                if isinstance(series, pd.DataFrame):
                    if len(series.columns) > 0:
                        series = series.iloc[:, 0]
                    else:
                        return None
                non_null = series.dropna()
                if len(non_null) > 0:
                    result = flatten_to_scalar(non_null.iat[0])
                    # æœ€ç»ˆå…œåº•ï¼šè‹¥ä»æ˜¯DataFrame/Seriesï¼Œå¼ºåˆ¶æå–æ ‡é‡
                    if isinstance(result, pd.DataFrame):
                        try:
                            return str(result.iloc[0, 0]) if result.shape[0] > 0 and result.shape[1] > 0 else None
                        except:
                            return str(result)
                    elif isinstance(result, pd.Series):
                        try:
                            return str(result.iloc[0]) if len(result) > 0 else None
                        except:
                            return str(result)
                    return result
                if len(series) > 0:
                    result = flatten_to_scalar(series.iat[0])
                    # åŒæ ·çš„å…œåº•é€»è¾‘
                    if isinstance(result, pd.DataFrame):
                        try:
                            return str(result.iloc[0, 0]) if result.shape[0] > 0 and result.shape[1] > 0 else None
                        except:
                            return str(result)
                    elif isinstance(result, pd.Series):
                        try:
                            return str(result.iloc[0]) if len(result) > 0 else None
                        except:
                            return str(result)
                    return result
                return None
            
            # æ„å»ºèšåˆè§„åˆ™
            agg_dict = {}
            for col in main.columns:
                if col in group_cols:
                    continue
                    
                if pd.api.types.is_numeric_dtype(main[col]):
                    # æ•°å€¼åˆ—ï¼šæ±‚å’Œ
                    agg_dict[col] = 'sum'
                else:
                    # æ–‡æœ¬åˆ—ï¼šä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°ï¼ˆé¿å…'first'çš„bugï¼‰
                    agg_dict[col] = safe_first
            
            # V3.5.8: èšåˆå‰é˜²å¾¡ - åˆå¹¶é‡å¤åˆ— + æ·±åº¦æ¸…ç†éæ ‡é‡
            print(f"  [èšåˆå‰è¯Šæ–­] åˆå¹¶é‡å¤åˆ—å¹¶æ¸…ç†éæ ‡é‡å€¼...")
            consolidate_duplicate_columns(main)
            cleaned_by_col, offenders_after = deep_clean_nonscalars(main, skip_cols=group_cols, verbose=True)
            # å†æ¬¡ç¡®è®¤æ— é‡å¤åˆ—
            from collections import Counter
            dup_after = [n for n, c in Counter(main.columns).items() if c > 1]
            if dup_after:
                print(f"  [è­¦å‘Š] åˆå¹¶åä»å­˜åœ¨é‡å¤åˆ—: {dup_after}")
            else:
                print(f"  [æ ¡éªŒ] æ— é‡å¤åˆ—")
            
            # æ‰§è¡Œèšåˆ
            main = main.groupby(group_cols, as_index=False).agg(agg_dict)
            print(f"  After aggregation: {len(main)} rows (grouped by date+platform+id)")
            
            # è°ƒè¯•ï¼šèšåˆåçš„æ•°æ®ç»Ÿè®¡
            if 'å……å€¼é‡‘é¢' in main.columns:
                print(f"  After agg - å……å€¼é‡‘é¢æ€»å’Œ: {main['å……å€¼é‡‘é¢'].sum():.2f}")
            
            main = main.sort_values(group_cols)
    
    # åªä¿ç•™ 53 åˆ—é¡ºåº
    main = main[FINAL_COLUMNS]

    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
    if len(main) == 0:
        print(f"[ERROR] æ¸…æ´—åæ²¡æœ‰ä»»ä½•æ•°æ®è¡Œï¼Œè·³è¿‡ç”Ÿæˆæ–‡ä»¶ã€‚")
        print(f"   è¯·æ£€æŸ¥æ•°æ®æºæ˜¯å¦åŒ…å«å¿…è¦çš„ã€æ—¥æœŸã€‘å’Œã€æ€»ä»£åç§°/æ¸ é“ã€‘åˆ—ã€‚")
        return

    # V3.5.9: ç”Ÿæˆç¼ºå¤±å­—æ®µè¯Šæ–­æŠ¥å‘Š
    print(f"\n[æ•°æ®è¯Šæ–­] ç”Ÿæˆç¼ºå¤±å­—æ®µæŠ¥å‘Š...")
    try:
        with open("missing_fields_report.txt", "w", encoding="utf-8") as f:
            f.write(f"=== æ¯æ—¥æ€»ä»£æ•°æ® - å­—æ®µè¯Šæ–­æŠ¥å‘Š ===\n")
            f.write(f"æŠ¥è¡¨æ—¥æœŸ: {selected_date if 'selected_date' in locals() else TARGET_DATE}\n")
            f.write(f"æ€»è¡Œæ•°: {len(main)}\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            for col in FINAL_COLUMNS:
                if col in main.columns:
                    if pd.api.types.is_numeric_dtype(main[col]):
                        non_zero = (main[col] != 0).sum()
                        total_val = main[col].sum()
                        f.write(f"{col}: {non_zero}/{len(main)} éé›¶ (åˆè®¡: {total_val:.2f})\n")
                    else:
                        non_empty = main[col].notna().sum()
                        f.write(f"{col}: {non_empty}/{len(main)} éç©º\n")
                else:
                    f.write(f"{col}: [ç¼ºå¤±åˆ—]\n")
        print(f"  âœ“ è¯Šæ–­æŠ¥å‘Šå·²ä¿å­˜: missing_fields_report.txt")
    except Exception as e:
        print(f"  âš ï¸ è¯Šæ–­æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")

    # V3.5.10: ç»Ÿä¸€æµ®ç‚¹æ•°ä¸ºä¸¤ä½å°æ•°
    print(f"\n[æ ¼å¼åŒ–] ç»Ÿä¸€æµ®ç‚¹æ•°ä¸ºä¸¤ä½å°æ•°...")
    float_cols = [c for c in main.columns if main[c].dtype == 'float64']
    if float_cols:
        main[float_cols] = main[float_cols].round(2)
        print(f"  âœ“ å·²æ ¼å¼åŒ– {len(float_cols)} ä¸ªæµ®ç‚¹åˆ—")

    # å†™å‡º Excel
    # V3.1: æ·»åŠ æ–‡ä»¶å†™å…¥å¼‚å¸¸å¤„ç†
    write_success = False
    final_output_path = output_path
    
    for attempt in range(3):  # å°è¯•3æ¬¡
        try:
            with pd.ExcelWriter(final_output_path, engine="openpyxl") as writer:
                main.to_excel(writer, sheet_name="DailyAgentData", index=False)
            write_success = True
            print(f"âœ“ æŠ¥è¡¨ç”ŸæˆæˆåŠŸ: {len(main)} è¡Œ, 53 åˆ—")
            print(f"âœ“ è¾“å‡ºæ–‡ä»¶: {final_output_path}")
            break
        except PermissionError as e:
            if attempt < 2:  # å‰ä¸¤æ¬¡å°è¯•ä½¿ç”¨å¤‡ä»½æ–‡ä»¶å
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                base_name = os.path.splitext(output_path)[0]
                ext = os.path.splitext(output_path)[1]
                final_output_path = f"{base_name}_backup_{timestamp}{ext}"
                print(f"  âš ï¸ æ–‡ä»¶è¢«å ç”¨ï¼Œå°è¯•å¤‡ä»½æ–‡ä»¶å: {final_output_path}")
            else:
                print(f"\nâŒ é”™è¯¯ï¼šæ–‡ä»¶å†™å…¥å¤±è´¥ï¼")
                print(f"   åŸå› ï¼š{e}")
                print(f"   è§£å†³æ–¹æ¡ˆï¼š")
                print(f"   1. å…³é—­æ‰€æœ‰æ‰“å¼€çš„Excelæ–‡ä»¶ï¼ˆç‰¹åˆ«æ˜¯ {os.path.basename(output_path)}ï¼‰")
                print(f"   2. å¦‚æœæ–‡ä»¶ä»è¢«å ç”¨ï¼Œè¯·é‡å¯Excelæˆ–é‡å¯ç”µè„‘")
                print(f"   3. ç„¶åé‡æ–°è¿è¡Œè„šæœ¬")
                raise
        except Exception as e:
            print(f"\nâŒ é”™è¯¯ï¼šæ–‡ä»¶å†™å…¥å¤±è´¥ï¼")
            print(f"   åŸå› ï¼š{e}")
            raise
    
    if not write_success:
        print(f"\nâŒ é”™è¯¯ï¼šæ— æ³•å†™å…¥æ–‡ä»¶ï¼")
        return
    
    # Save summary to file
    with open("generation_summary.txt", "w", encoding="utf-8") as f:
        f.write(f"Report: {os.path.basename(output_path)}\n")
        f.write(f"Rows: {len(main)}\n")
        f.write(f"Columns: {len(main.columns)}\n")
    print("Summary saved to: generation_summary.txt")


if __name__ == "__main__":
    pd.set_option("future.no_silent_downcasting", True)
    parser = argparse.ArgumentParser(description="ç”Ÿæˆæ¯æ—¥æ€»ä»£æ•°æ®æŠ¥è¡¨")
    parser.add_argument("--input", "-i", default=".", help="è¾“å…¥ç›®å½•ï¼ˆæ”¾ç½®æ‰€æœ‰æ•°æ®æºçš„æ–‡ä»¶å¤¹ï¼‰")
    parser.add_argument("--output","-o", default=f"æ¯æ—¥æ€»ä»£æ•°æ®_è‡ªåŠ¨ç”Ÿæˆ.xlsx", help="è¾“å‡ºExcelè·¯å¾„")
    parser.add_argument("--date", "-d", default=None, 
                       help='ç›®æ ‡æ—¥æœŸï¼ˆå¦‚ "2025-10-27"ï¼‰ã€‚ä¸æŒ‡å®šæˆ–ä½¿ç”¨ "latest" åˆ™è‡ªåŠ¨ä½¿ç”¨æœ€æ–°æ—¥æœŸ')
    args = parser.parse_args()
    
    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æˆ–é…ç½®æ–‡ä»¶ä¸­çš„ TARGET_DATE
    target_date = args.date if args.date else TARGET_DATE
    
    main(args.input, args.output, target_date)
